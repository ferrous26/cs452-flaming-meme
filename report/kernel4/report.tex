\documentclass[pdftex,10pt,a4paper]{article}
\usepackage{../cs452}
\usepackage{verbatim}

\begin{document}

\kernelmake{4}

\section*{Overview}

This kernel development milestone adds the final set of kernel
features required for train control, namely serial I/O for both
the terminal and train controller. To demonstrate the finished kernel,
the M{\"a}rklin train set can be controlled by user input
to a terminal, and train track state is updated live on screen as the
trains travel around the track.

Commands issued into the terminal can manipulate trains and
switches, as well as monitor sensors. Trains can start, stop, toggle
lights and sound effects, and adjust speed. Switches can be toggled or
explicitly set to the straight or curved state.

The terminal displays information about current state
of the trains and switches, including a short history of the most
recent sensor activations. However, sensor activity cannot be
controlled by the user in any way.


\section*{Operation Instructions}

A pre-compiled kernel exists at
\ttt{/u/cs452/tftp/ARM/marada/k4.elf}, which can be loaded with
RedBoot using the following command:

\begin{center}
  \ttt{load -h 10.15.167.4 ARM/marada/k4.elf; go}
\end{center}

Notice that no offset should be specified for the load instruction.

The source code for the kernel exists in \ttt{/u3/marada/kernel4/},
and can be compiled with the following command chain:

\begin{center}
  \ttt{cd /u3/marada/kernel4 \&\& /u/cs444/bin/rake local}
\end{center}

Which will produce a \ttt{kernel.elf} file in the same directory as
the makefile, built in release mode with benchmarking enabled.

After Initialization the kernel will print
``\ttt{Welcome to Task Launcher}'' into the message logging area of
the screen and a command prompt will appear near the center of the
screen titled ``TERM> ''. At this point commands can be entered; keys
pressed should be echoed back at the command prompt in place of the
white cursor. Pressing the return key will confirm a command and cause
it to be processed. An online listing of the commands can be printed
by entering an empty command.

\subsection*{Submitted Files}
\begin{center}
\begin{tabular}{l|l}
  \bfseries File & \bfseries MD5 Hash
  \\\hline
  \csvreader[head to column names]{md5_info.csv}{}%
  {\\\file & \ttt{\hash}}%
\end{tabular}
\end{center}

\newpage
\part*{The Kernel}

The kernel is organized into four sections: task management,
scheduling tasks, system calls between tasks, and handling interrupt
driven hardware events.

\section*{Task Management}

Task metadata is stored in a static array of task descriptors. The
array holds 64 descriptors, allowing for up to 64 concurrent tasks to
run on our system. The constant \ttt{TASK\_MAX} is used to represent
the maximum number of concurrent tasks and is used by various other
components of the system to provide upper limits on data structures
and calculations.

Each task descriptor, defined as type \ttt{task}, tracks the following
task metadata:

\begin{description}
  \item[\ttt{int tid}] \hfill \\
    Task identifier. A globally unique identifier for the task. The
    identifier is stored as a signed integer as it is the simplest way
    to maintain consistency with the required API for the
    \ttt{Create} system call.

    The task identifier value is tied to the array index of the
    descriptor so that no two live tasks can have the same task
    identifier. When a task \ttt{Exit}s the descriptor will increment
    the \ttt{tid} by \ttt{TASK\_MAX} so that the next task to use the
    descriptor will not reuse a previously allocated task
    identifier. This scheme also allows us to calculate the index of
    the descriptor in a single instruction given the task's identifier.

    No safety is provided at the point where the task identifier
    values overflow, though we do not have any use cases which
    approach the possibility of causing an overflow.

  \item[\ttt{int p\_tid}] \hfill \\
    Parent task Identifier. A copy of the \ttt{tid} of the task which
    \ttt{Create}d the task occupying the descriptor. This value is a
    copy because the only time that the parent is guaranteed to be
    alive concurrently with the child is during the \ttt{Create} call
    that allocates the child task. The first task that is run by our
    kernel has a \ttt{p\_tid} value of its own \ttt{tid}---the task is
    effectively its own parent.

  \item[\ttt{int priority}] \hfill \\
    Priority level. Valid priority values are between $0$ and $31$,
    with $31$ being the highest priority and $0$ being the lowest
    priority. This range of priorities is efficiently represented by a
    single word bitmap during scheduling. Though this value
    does not require the full 32 bits of space provided by an integer,
    storing it as a full word sized value makes memory access easier
    (faster) for the hardware.

    We have defined constants for some priority levels, such as
    \ttt{TASK\_PRIORITY\_EMERGENCY} for level $31$ and
    \ttt{TASK\_PRIORITY\_MEDIUM} for level $15$.

  \item[\ttt{task* next}] \hfill \\
    Next task. This meaning of this value depends on the state of the
    task.

    During message passing the value is either a valid pointer
    to another task descriptor, \ttt{NULL}, or it contains a special
    value that indicates that the process is blocked.

    During all other times, the \ttt{next} pointer will contain a
    valid pointer to the next task of the same \ttt{priority} which is
    ready to run, or \ttt{NULL} if no such task exists.

  \item[\ttt{int* sp}] \hfill \\
    Task stack pointer. This value is \ttt{NULL} for descriptors which
    are not allocated to live tasks. When belonging to a live task,
    this value points to the top of the stack for the task at the
    point when the task execution was last interrupted. The top of the
    stack will conveniently also point to the trap frame for the task.
\end{description}

\newpage

Task \ttt{state} is not explicitly stored in a task descriptor because
our kernel implementation only needs to know state in cases where the
\ttt{next} pointer or \ttt{sp} pointer will not be used. The
\ttt{next} pointer and \ttt{sp} pointer are overloaded to contain
\ttt{state} information in a way that has no performance overhead for
task scheduling and such that we only need to check exactly one of
those values when interested in a particular state.


\subsection*{Task Initialization}

Task creation and initialization is implemented in the
\ttt{task\_create} function.

To get a descriptor, we consume a value from the free list, which
is a circular buffer of descriptor indices. If the free list is empty
then no descriptors are available and an error code is returned
indicating the problem.

The descriptor will already have the correct \ttt{tid} value, either
set during system initialization or during task cleanup from the
previous user of the descriptor.

The \ttt{p\_tid} is set using the calling task's \ttt{tid}. The
\ttt{priority} value is set using an argument passed to the task
creation function which would normally map to the same value passed to
the \ttt{Create} system call.

The initial \ttt{sp} is then set and the default trap frame is
initialized in place on the task's stack. A task descriptor will
always be given the same initial stack pointer based on the index of
the descriptor.

The default trap frame sets the correct values for the program
counter, stack, and CPSR registers. Notably, the default link register
for a task is a pointer to the \ttt{Exit} system call, which ensures
that any task that does not explicitly call \ttt{Exit} instead of
simply \ttt{return}ing will still cleanup properly.

Finally, the task is given to the scheduler and scheduled to run.

\subsection*{Task Cleanup}

Task cleanup is implemented in the \ttt{task\_destroy} function.

A task is only cleaned up when it calls \ttt{Exit} and enters the
\ttt{ZOMBIE} state.

First, the \ttt{tid} of the descriptor is updated for the next
descriptor occupant. Then the \ttt{sp} is set to \ttt{NULL} to
indicate that the descriptor does not contain a live task.

Then the receive queue, used in message passing, must be flushed in
order to unblock any tasks that were trying to send a message to
now-zombified task.

Finally, the descriptor is added to the end of the free list so that
it can be allocated to a new task.


\section*{Scheduling}

Task scheduling is implemented in the \ttt{schedule\_schedule}
function.

The task scheduler is implemented as a bit field, which maps bits to
priority levels, and an array of \ttt{head} and \ttt{tail} pointers,
which are used for priority queues. The \ttt{next} pointer in the task
descriptors is also used for priority queues when needed.

When a task is scheduled, the scheduler is given a pointer to the
descriptor of the task to be scheduled. With the descriptor, the
\ttt{priority} level can be looked up, and the correct bit in the bit
field can be turned on to indicate a waiting job at that priority
level.

The correct pair of \ttt{head} and \ttt{tail} pointers are also looked
up using the \ttt{priority}. Using the value of the \ttt{tail}
pointer, the descriptor that is currently at the end of the list can
have its \ttt{next} pointer updated to point to the task being
scheduled. However, if there were no other task in the queue, then
the \ttt{head} pointer is set instead.

The \ttt{tail} pointer is always set to the task that was scheduled,
and the \ttt{next} pointer of the task being scheduled is always set
to \ttt{NULL} to indicate the end of the list.

By using the \ttt{next} index of the descriptor we saves the need to
allocate a full queue for each priority level. The \ttt{next} pointers
are also used for messaging queues to provide even more memory
savings.

\subsection*{Activation Selection}

Activation selection is implemented in the \ttt{scheduler\_get\_next}
function.

When the scheduler is asked to find the next task to be activated
during a system call or hardware interrupt, it will look for the
highest bit set in the bit field to select the priority queue and look
up the \ttt{head} pointer for that queue and then retrieve the correct
task descriptor.

Then, the \ttt{head} pointer is moved up using the \ttt{next} pointer
of the \ttt{head} task. If \ttt{next} was \ttt{NULL} then we know that
the queue is empty and the bit in the bit field should be turned off.

Finally, the \ttt{task\_active} global, which is a pointer to the
currently active task will be set to the selected task.

\section*{System Calls}

System calls are implemented in the standard way using the \ttt{swi}
instruction. Arguments, including the system call type, are passed
into the kernel via a pointer to a request object. The request object
itself lives on the stack of the task making the system call.

After completing the system call request, the calling task is
rescheduled and the next task to be run is activated.

The software interrupt code path is implemented in a totally
non-reentrant way which requires that hardware interrupts be disabled
while in the kernel. However, this did allow us to make certain
optimizations that shorten the time spent in the kernel for system calls.

As a general pattern, we try to perform argument checking outside of
the kernel in order to reduce the time that is spent inside of the
kernel where hardware interrupts are ignored.

\subsection*{Software Context Switch}

Software system calls are invoked via the wrapper function
\ttt{\_syscall()}. The reason we use this is so we can ensure the the
parameters are placed in the correct registers when the \ttt{swi} call
is performed. Another advantage to this is that GCC will ensure that
registers \ttt{r0-r3} as well as the \ttt{ip} are required for any further
task execution will be backed up.

At the time when the user task is rescheduled, the return value from
the system call will be in \ttt{r0}, which follows the standard
function calling convention.

The immediate value associated with the \ttt{swi} instruction is not
used. Instead, we pass the system call number via \ttt{r0}. Additional
arguments for the system call are stored in a \ttt{kernel\_req}uest
structure on the task's stack and a pointer to the structure is left
in \ttt{r1}.

When the system call jump is performed the \ttt{spsr} and \ttt{lr} are
placed into \ttt{r3} and \ttt{r2} respectively. After this, all of the
registers are saved onto the task's stack so that they can be reloaded
when the task is rescheduled. At this point the kernel stack pointer
is reset and the function \ttt{syscall\_handle} is invoked. The first
argument will be the system call code, the second argument is the
pointer to the \ttt{kernel\_req}uest structure, and the third argument
is the stack pointer of active user task. This is all that is needed
by the kernel to service the user task and maintain all of the task's
state.

On kernel exit the \ttt{r0} register will be used by the kernel to write
back the return value of the system call, and \ttt{r2} and \ttt{r3}
registers will hold information required to restore the user back to
the state.

Since our kernel is not designed to be reentrant, we do not save
any of the kernel's working registers. Every time we enter the kernel from
an interrupt, just before the branch into \ttt{syscall\_handle}, the
kernel's \ttt{sp} is reset to the value \ttt{0x300000}, which is
reserved space for the kernel stack. This decision has also allowed us
to make \ttt{syscall\_handle} a naked function. Combined, these
optimizations eliminate relatively large amounts of redundant work
from context switching making system calls faster and reducing the
amount of time spent in the kernel.


\subsection*{Core System Calls}

\subsubsection*{int Create(int priority, void (*code)(void))}

\begin{description}
\item[\ttt{priority}] the priority level of the task to be created
\item[\ttt{code}] pointer to the function where the new task will begin
\end{description}

Create and schedule a new task.

Returns the \ttt{tid} of the created task if successful, otherwise
negative corresponding to one the following errors:

\begin{description}
\item[\ttt{NO\_DESCRIPTORS (-1)}] No task descriptors are available for
  allocation
\item[\ttt{INVALID\_PRIORITY (-2)}] Priority level is negative or greater
  than \ttt{TASK\_PRIORITY\_MAX}
\end{description}


\subsubsection*{int myTid(void)}

Fetch the task identifier for the calling task.

Returns the \ttt{tid} of the calling task. This system call cannot
fail (except for maybe if there is a catastrophic hardware failure).


\subsubsection*{int myParentTid(void)}

Fetch the task identifier for the calling task's parent task.

Returns the \ttt{p\_tid} of the calling task. This system call cannot fail.


\subsubsection*{int myPriority(void)}

Fetch the task priority for the calling task.

Returns the \ttt{priority} of the calling task. This system call cannot fail.


\subsubsection*{int ChangePriority(int priority)}

\begin{description}
\item[\ttt{priority}] the new priority level for the calling task
\end{description}

Change the priority level of the calling task.

Returns the \ttt{priority} of the calling task. This system call cannot fail.


\subsubsection*{void Pass(void)}

This system call effectively does nothing. However, it can be used to
give up execution to the next task to run.


\subsubsection*{void Exit(void)}

Cease execution of the calling task. This system call does not return
and the descriptor for the calling task is cleaned. This system call
cannot fail.


\subsubsection*{void Abort(char* file, unsigned int line, char* msg, \ldots)}

Cease execution of the entire system and print out diagnostic
information, including the format string given by \ttt{msg}.

This system call is not usually called directly, but is instead called
by an assertion failure. Output is generated in the kernel to avoid
being interrupted by hardware and output using busy-wait I/O.

In addition to source information and the format string, output will
include a full dump of the kernel state. Kernel state includes the
descriptor table and information about tasks waiting on
interrupts. The tableau can be used to reconstruct what was going on
inside of the kernel, and, to a limited extent, what was going on in
each running task.

This system call cannot fail (unless the failure also causes the
kernel to become severely corrupted).


\subsubsection*{void Shutdown(void)}

Cease execution of the system. Unlike the \ttt{Abort} system call,
this exit does not include any debugging information and is meant to
be used for cleanly shutting down the system.

This system call does not return and cannot fail.


\subsection*{Message Passing}

Message sending is implemented using messaging queues. The queue
implementation uses the same logic as that of the priority queues for
scheduling, but the producers and consumers of the queue are
different. The instance data for both queues have some overlap as they
share use of the \ttt{next} pointer in task descriptors for their
queueing.

When a message is sent using the \ttt{Send} system call, the
descriptor enters the receive queue for the receiving task. If the
receiving task was already waiting for the message, then it is given
the message right away and woken up.

When a task wants to receive a message it uses the \ttt{Receive}
system call to check if there are any messages in its receive
queue. If a message is available it consumes the message from the
queue and sets the message sender into the \ttt{RPLY\_BLOCED} state.
If no message is available then it itself as being blocked waiting for a
message by setting its own \ttt{next} pointer to \ttt{RECV\_BLOCKED}.

The \ttt{RPLY\_BLOCKED} state is marked using the \ttt{next} pointer
of the message sender. Only tasks in the \ttt{RPLY\_BLOCKED} state can
be replied to using the \ttt{Reply} system call.

After a message has been received and processed, the receiver can
reply using the \ttt{Reply} system call. The \ttt{Reply} call must be
to a task in the \ttt{RPLY\_BLOCKED} state or it will fail.

In this system, we do not need to know if a task is
\ttt{SEND\_BLOCKED}, but we can still determine if a task
\ttt{SEND\_BLOCKED}, for debugging, by searching through the receive
queues.

The \ttt{RECV\_BLOCKED} and \ttt{RPLY\_BLOCKED} state are represented
as pointers to low memory addresses. The low memory addresses are
guaranteed to be invalid pointers to data from the kernel. This makes
it easy and reliable to have assertions in the kernel regarding task
state. The small addresses also can be represented in any assembly
instruction immediate slot, so it is also efficient.

Message passing is implemented in a simple way with minimal
overhead. A 4 byte message can be sent and replied to with another 4
byte message in approximately $\sim6.0\mu$seconds.


\subsubsection*{int Send(int tid, char* msg, int msglen, char* reply, int replylen)}

\begin{description}
\item[\ttt{tid}] The task identifier of the receiver of the message
\item[\ttt{msg}] A pointer to the buffer containing the message
\item[\ttt{msglen}] The size, in bytes, of the buffer containing the message
\item[\ttt{reply}] A pointer to the buffer where the message reply can
  be placed
\item[\ttt{replylen}] The size, in bytes, of the \ttt{reply} buffer
\end{description}

Send a message to another task. Using this function, a task can send an
arbitrary message (possibly an empty message) to any task in the
system except for itself. The message can be any data structure, it
does not need to be a string.

The task will remain blocked until it receives a \ttt{Reply}. The
\ttt{reply} buffer will contain the reply when the call
returns. However, the \ttt{reply} might be empty and the return value
must be checked to know the size of the reply.

If a task sends a message to itself it will deadlock itself. In debug
builds of the kernel this case is asserted against. However, release
builds do not include this check as it is an incredibly unlikely case
should have been caught during testing with debug builds.

If the receiver never receives the message then the sender will block
until the receiver \ttt{Exit}s.

If the receiver does not provide a receive buffer at least
\ttt{msglen} bytes in size then the sender will be rejected with an
appropriate error code.

This function returns the actual size of the \ttt{reply} if the
message send was successful, otherwise a negative value is returned
that corresponds to one the following errors:

\begin{description}
\item[\ttt{IMPOSSIBLE\_TASK (-1)}] The \ttt{tid} argument was negative
  and therefore impossible
\item[\ttt{INVALID\_TASK (-2)}] The \ttt{tid} argument does not
  correspond to a task this is currently alive
\item[\ttt{INCOMPLETE (-3)}] The receiver of the message died before
  receiving the message
\item[\ttt{NOT\_ENUF\_MEMORY (-4)}] The receiver did not provide a
  receive buffer large enough for \ttt{msg}
\end{description}


\newpage
\subsubsection*{int Receive(int* tid, char* msg, int msglen)}

\begin{description}
\item[\ttt{tid}] The task identifier of the sender of the message
\item[\ttt{msg}] A pointer to the buffer where to place the message
\item[\ttt{msglen}] The size, in bytes, of the buffer for the message
\end{description}

Receive a, possibly empty, message from another task. This function
will block until a message is received. The received message will be
placed into the \ttt{msg} buffer that was given and the task
identifier of the message sender will be copied into \ttt{tid}.

If the message is larger than \ttt{msglen} then the sender will be
refused an the receiver will continue to block waiting for a message.

This function returns the actual size of the \ttt{msg} if the
message send was successful. There are no error codes returned by this
function as error handling is forced onto the message sender.


\subsubsection*{int Reply(int tid, char* reply, int replylen)}

\begin{description}
\item[\ttt{tid}] The task identifier of the receiver of the message
\item[\ttt{reply}] A pointer to the buffer with the reply message
\item[\ttt{replylen}] The size, in bytes, of the \ttt{reply} buffer
\end{description}

Reply to a message sender. Using this function, a task can unblock a
task that is \ttt{RPLY\_BLOCKED} waiting for a message
reply. Technically, any task can reply to a message, it does not have
to be the receiver of the message. However, in our kernel message
passing has only ever been between two tasks.

As with \ttt{Send}, the reply can be any arbitrary message, even an
empty message is permitted.

The task will not get blocked trying to reply. If the reply message
cannot be copied into the replyee's buffer then an error code is
returned.

This function returns \ttt{OK} ($0$) to indicate that the reply was
successful and the replyee has been unblocked. If an error occurred,
then a negative value is returned that corresponds to one of the
following errors:

\begin{description}
\item[\ttt{IMPOSSIBLE\_TASK (-1)}] The \ttt{tid} argument was negative
  and therefore impossible
\item[\ttt{INVALID\_TASK (-2)}] The \ttt{tid} argument does not
  correspond to a task this is currently alive
\item[\ttt{INVALID\_RECVER (-3)}] The replyee is not \ttt{RPLY\_BLOCKED}
\item[\ttt{NOT\_ENUF\_MEMORY (-4)}] The replyee did not provide a
  receive buffer large enough for \ttt{msg}
\end{description}

\subsection*{Hardware Events}

All hardware events are handled though the use of a single system call. Since
the interface of the call allowed us to be quite flexible with what is passed
to the kernel to handle the event it was was to ensure that user land never
needs to ever access hardware directly.

\subsubsection*{int AwaitEvent(int eventid, char* event, int eventlen)}

\begin{description}
\item[\ttt{eventid}] An identifier for the event action to wait upon
\item[\ttt{event}] A pointer to the buffer for use with a particular event
\item[\ttt{eventlen}] The size, in bytes, of the \ttt{event} buffer
\end{description}

We chose this signature for use in the await event signature since it allowed
for us to perform actions that could send or return more than a word of data
at a time. This is quite handily when having to deal with performing actions
with the FIFO's that are enabled on \ttt{UART2}. The action that is performed
on the buffer is different depending on which event is being waited upon.

The functions return differs depending on the event that is waited upon but a
good rule of thumb is that on success it will return how many characters in the
buffer have been processed. For failures we do not use any error codes since
they are all directly from the misuse of the function. Instead of an error
being thrown we shut down the kernel and notify the user what was wrong with
the await call so the user can then go fix it.


\subsubsection*{int Delay(int ticks)}

\begin{description}
\item[\ttt{ticks}] The number of clocks ticks to delay the task
\end{description}

Suspend execution of the the caller task for \ttt{ticks} clock
ticks. The actual delay is guaranteed to be at least \ttt{ticks} clock
ticks, but may be longer if the system is under heavy load.

Negative and zero time delay values for \ttt{ticks} are checked by the
system call function before context switching into the
kernel. Negative or zero delays are interpreted as an indicator that
the system is missing deadlines and by checking the value before
calling into the kernel we save some time by not having to actually
pass the message to the clock server. In this case an error code value
is returned. However, we plan to add some sort of visual notifier
for this case in the future as it is important for debugging.

A successful delay will return the current system time, a positive
value. A failure in the message sending will cause a negative integer
value to be returned. The error codes correspond to that of the
\ttt{Send} system call or are one of the following:

\begin{description}
\item[\ttt{INVALID\_DELAY (-60)}] The \ttt{ticks} value was less or
  equal to $0$
\end{description}


\subsubsection*{int DelayUntil(int ticks)}

\begin{description}
\item[\ttt{ticks}] The system time which the task should be delayed
  until
\end{description}

Suspend execution of the caller task until the system time is \ttt{ticks}.
The actual delay is guaranteed to be at least until \ttt{ticks}
clock ticks for the system, but may be longer if the system is under
heavy load.

Negative and zero time delay values for \ttt{ticks} are checked by the
system call function before context switching into the
kernel. However, unlike \ttt{Delay}, this value cannot be fully
checked without knowing the current system time, which is not known
until the clock server has the request.

A successful delay will return the current system time, a positive
value. A failure in the message sending will cause a negative integer
value to be returned. The error codes correspond to that of the
\ttt{Send} system call or are one of the following:

\begin{description}
\item[\ttt{INVALID\_DELAY (-60)}] The \ttt{ticks} value was less or
  equal to $0$
\end{description}


\subsubsection*{int Time()}

Returns the current system time.

A successful call will return the current system time, a positive
value. A failure in the message sending will cause a negative integer
value to be returned. The error codes correspond to that of the
\ttt{Send} system call.


\subsubsection*{int Putc(int channel, char c)}

\begin{description}
\item[\ttt{channel}] The serial channel to print the character to
\item[\ttt{c}] The character to print
\end{description}

Print a character to an arbitrary serial channel. This call will block
the caller until the character has been buffered in the serial server.

The only valid values for \ttt{channel} are \ttt{TERMINAL} ($2$) and
\ttt{TRAIN} ($1$). Any other value for \ttt{channel} will result in a
compile time error.

In the case of the \ttt{TERMINAL} channel, this function is a wrapper
for the \ttt{Puts} system call.

A successful \ttt{Putc} will return \ttt{OK}. A failure in the message
sending will cause a negative integer value to be returned. The error
codes correspond to that of the \ttt{Send} system call.


\subsubsection*{int Getc(int channel)}

\begin{description}
\item[\ttt{channel}] The serial channel where to get the character from
\end{description}

Get a character from an arbitrary serial channel. This call will block
the caller until a character is available from the given \ttt{channel}.

The only valid values for \ttt{channel} are \ttt{TERMINAL} ($2$) and
\ttt{TRAIN} ($1$). Any other value for \ttt{channel} will result in a
compile time error.

A successful \ttt{Getc} will return the input character, a positive
value. A failure in the message sending will cause a negative integer
value to be returned. The error codes correspond to that of the
\ttt{Send} system call.


\subsubsection*{int Puts(char* str, int length)}

\begin{description}
\item[\ttt{str}] The string to be printed to the terminal
\item[\ttt{length}] The length of the string to be printed to the terminal
\end{description}

Print a string to the terminal. This call will block the caller until
the string has been buffered in the terminal server.

Format strings and control sequences should be buffered in a string in
the calling task using the \ttt{sprintf} family of functions defined
in \ttt{vt100.h}.

Strings are not assumed to be null terminated and the given
\ttt{length} is taken as the strings length. As we use \ttt{sprintf}
for formatting strings, we are able to calculate the string length as
it is being formatted and can easily pass the length to the system
call.

A successful \ttt{Puts} will return \ttt{OK}. A failure in the message
sending will cause a negative integer value to be returned. The error
codes correspond to that of the \ttt{Send} system call.


\section*{Hardware Interrupts}

When we were designing our interrupt code we wanted the user space tasks to
have no direct interaction with the hardware. This philosophy was chosen since
it allowed us to make our user land easier to program in as well as allowing us
ensure that while interacting with the hardware an interrupt can not cause us
to perform an action that no longer reflects the state of the system.

\subsection*{Hardware Context Switch}

The Hardware interrupt code is built such that it encapsulates the software
interrupt. Since the software interrupt does not save scratch registers that
GCC will assume have been clobbered during a function execution the hardware
interrupt first saves those values and writes a flag into the position of the
old link register to let the exit code know that it needs to perform the extra
hardware interrupt exit code. This decision was made because it allows for us
to reuse all of our existing assembly and required for a very small amount of
change to the existing software interrupt entry code.

When we enter the kernel we will still enter into the function
\ttt{syscall\_handle()}. When we are servicing an interrupt the interrupt code
value will get set to 0. At this point we save save the \ttt{sp} like normal
and can perform the hardware interrupt similarly as if it were regular software
interrupt reusing the same schedule and exit path.

\subsection*{Vectored Interrupt Controller (\ttt{VIC})}

In order to handle interrupt priority and lookup we employ use of the vectored
interrupt controllers \ttt{VEC1} and \ttt{VEC2}. They are set up in a daisy
chained configuration such that the priority of any action on \ttt{VEC1} will
take priority over the actions on \ttt{VEC2}. This was the nature of hardware
and not a decision that was made by us. The decision to use the \ttt{VIC}s was
because it allowed us to greatly simply our interrupt handling code by
offloading a non-trivial amount of work onto the available hardware. As well
as the code simplification we also get increased performance as the work
required to get the appropriate interrupt service routine can be performed in
parallel to the program execution.

When we enter the kernel to handle a hardware interrupt we get the interrupt
service routine by reading the function pointer off of the \ttt{VEC1}
controller. After the routine had been run we write back to the controller to
notify it that the interrupt has been serviced and it can get the next
interrupt service routine.

The largest issue is that when interrupt states are toggled between being
enabled or disabled the interrupt controller can get out of sync and will try
to handle the newly disabled interrupt though the default interrupt routine.
We were able to work around this by using a void function that does nothing but
return allowing for us to retrieve the next interrupt service routine from the
\ttt{VIC}.

\subsection*{\ttt{UART1} Events}
To interact with \ttt{UART1} we enable and use a combination of 3 different
interrupts. The reason that we use 3 different interrupts instead of just the
\ttt{UART1} or interrupt is because with the use of the \ttt{VIC} we can make
assumptions of the state of \ttt{UART1} based on which ISR gets run simplifying
the \ttt{UART} handling code greatly.

The interrupts are:
\begin{description}
\item[Interrupt 23 - \ttt{UART1} Receive] \hfill \\
	This is the Interrupt that is enabled with the highest priority on
	\ttt{VEC1}. The reason for this is because it has the smallest window
	of time before the byte in the hold register is lost so we ensure that
	it will always be handled first.
\item[Interrupt 24 - \ttt{UART1} Send] \hfill \\
	This interrupt has the second lowest priority on \ttt{VIC1}. This is
	because most actions that are going to be sent to the train controller
	are time sensitive and should go out when possible. However, the
	interrupt its self does not require for us to handle it before some
	amount of time has passed.
\item[Interrupt 52 - \ttt{UART1} General] \hfill \\
	This Interrupt is an or of all of the interrupt cases of \ttt{UART2}.
	Since the \ttt{UART1} Send and Receive interrupts are of a higher
	priority if the ISR for this interrupt gets invoked then it should be
	because of a modem interrupt. This Interrupt is handled with the
	highest priority on \ttt{VEC2} due to the requirement that the events
	of this interrupt are handled as fast as possible in order to ensure
	that the \ttt{UART1} send carrier can maintain proper flow control
	with the train controller.
\end{description}

These 3 events get combined into 4 separate events used to drive all IO to the
train set. Since we do not use the FIFO's and flow control to the train is done
via a clear to send bit the general interrupt is used to pick up the modem
interrupts.

A \ttt{UART1\_SEND} is hooked up to the \ttt{UART1} send interrupt. The user
task passes in the character that they would like to send to the train set
in the event call and execution will resume after the character has been
forwarded to the train set.

A \ttt{UART1\_RECV} is hooked up directly to the \ttt{UART1} send interrupt.
The user task provides a buffer for the train set to push a character into.
When a character is received for the train execution will continue with the
train input placed into the provided buffer.

Finally there are the {UART1\_DOWN} and {UART1\_CTS} events. These events
are both hooked up to the \ttt{UART} modem interrupt and are used to notify
the application that the clear to send bit has changed to off or on
respectively.

\subsection*{\ttt{UART2} Events}
Similarly, \ttt{UART2} uses a combination of 3 different interrupts:
\begin{description}
\item[Interrupt 25 - \ttt{UART2} Receive] \hfill \\
	This is the Interrupt that is enabled with the second highest priority
	on \ttt{VEC1}. The reason for this is because it is the only other
	interrupt inside of \ttt{VEC1} that if we do not service it in an
	applicable amount of time will also cause an error in the operation of
	the hardware. Due to the use of the FIFO we have quite a bit more time
	before bytes start getting lost.
\item[Interrupt 26 - \ttt{UART2} Send] \hfill \\
	This interrupt is enabled with the lowest priority inside of
	\ttt{VIC1}. The reason for this is because the terminal has a high
	enough bandwidth that the throughput to it only seems to be an issue
	in extraordinary circumstances and the interrupt its self is not time
	sensitive with regards to when it needs to be fulfilled.
\item[Interrupt 54 - \ttt{UART2} General] \hfill \\
	This Interrupt is an or of all of the interrupt cases of UART2.
	Since the \ttt{UART2} Send and Receive interrupts are of a higher
	priority if the ISR for this interrupt gets invoked then it should
	be because of a receive timeout interrupt. This interrupt is handled
	with the lowest priority of \ttt{VEC2} since if too many characters
	get into the receive queue then the \ttt{UART2} receive interrupt
	will then go off instead. With this guarantee we know that at this
	point it is not pertinent to clear the receive queue.
\end{description}

These three interrupts can be combined into 2 different events that are used to
perform all of the tasks required by the terminal server. Since we use the FIFO
queue on the \ttt{UART} we need to ensure that when less than 8 characters are
are placed into the buffer that we will still make them available to the system.

A \ttt{UART2\_RECV} event will wake up a task after data has been picked up
from \ttt{UART2}. The data is placed into the buffer provided by the task and
returns to number of character that have been placed into the tasks buffer. If
the \ttt{UART} has more characters than buffer space is available then the event
will only fill the buffer hoping that a task will wait on the event again
before the FIFO overflows.

A \ttt{UART2\_SEND} event takes a buffer from a task and will place up to 8
characters into the \ttt{UART2} FIFO queue. If less characters are available then
all of the characters will get placed into the queue. This event will return
the number of characters it was able to successfully place into the \ttt{UART}
before it rescheduled the user task.

\subsection*{Clock Events}

The clock only requires the use of one interrupt:
\begin{description}
\item [Interrupt 51 - Clock Underflow] \hfill \\
	This interrupt is run at a priority between the 2 \ttt{UART} priorities.
	This decision was because the timer has a relatively large window of
	time before we need to reset the interrupt state.
\end{description}

The clock only has one event that is triggered every time that the clock
underflow interrupt is thrown. If there is no task to wake up then we know
that a clock tick will have been missed and will log a message with the system.


\section*{Tasks}

List all the different tasks we have, their purpose, and the priority
that they run at.

In the case of server tasks, we need to refer back to the wrapper
functions on how to interact.

\subsection*{Clock Server}

The clock server is implemented in three parts. The first part, the
\ttt{clock\_notifier}, notifies the second part, the
\ttt{clock\_server}, on clock ticks. The third component is the UI for
the clock, which is implemented as a client of the
\ttt{clock\_server}.

\subsubsection*{Clock Notifier}

The \ttt{clock\_notifier} starts by asking for \ttt{myParentTid()} to
find out the task identifier of the \ttt{clock\_server}. It then
registers itself as ``CLOCK\_NOTE'' with the name server.
Then it enters a \ttt{FOREVER} loop where it \ttt{Await}s for
\ttt{CLOCK\_TICK} events from hardware and notifies the
\ttt{clock\_server} with a \ttt{Send} message whenever it
is woken up. The notifier runs at priority level 25, higher than any
non-notifier task, in order to mitigate the risk of missing a clock
tick event.

\subsubsection*{Clock Server}

The \ttt{clock\_server} starts by registering itself with the name
server under the name ``CLOCK''. It then \ttt{Create}s the
\ttt{clock\_notifier} and the \ttt{clock\_ui} task.

After setting everything up, the \ttt{clock\_server} enters a
\ttt{FOREVER} loop and begins \ttt{Receive}ing messages from client
tasks. The primary client task is the \ttt{clock\_notifier}, which
causes the server to increment the \ttt{time} and check if any tasks
waiting for \ttt{Delay}s need to be woken up.

The server keeps track of tasks that wish to be \ttt{Delay}ed in a
heap based priority queue. The queue is large enough to store the
maximum number of concurrent tasks for the system. A priority queue
allows us to check if any task needs to be woken up in constant
time. A heap implementation, combined with our hypothetical use case
for the clock server, also has the property that new \ttt{Delay}
requests can be inserted into the queue in near constant time in the
average case, or at least less than logarithmic time in many
cases. All other operations occur in logarithmic time, which we expect
to be sufficient for small and medium work loads.

We had considered an alternative implementation for the priority queue
based on sorted doubly linked lists, as the only non-constant time
operation would have been to add a new element to the list. However,
the performance did not appear to improve, possibly because of the
overhead of dealing with allocating nodes for the queue.

\subsubsection*{Clock UI}

The \ttt{clock\_ui} task prints the current time to the screen
periodically. After each printing, the task calls \ttt{Delay(10)},
which will delay the task for 10 clock ticks, which is equivalent to
$0.1$ seconds. The UI task will synchronize its time with the
\ttt{clock\_server} once per minute to make sure that the displayed
time does not drift from the stored time.


\subsection*{Name Server}

The Nameserver is the first task that is started up on the
initialization of task launcher. Since the Nameserver is pivotal to
the proper operation of the OS if something goes wrong with the
initialization of the Nameserver the task launcher will abort causing
the kernel to terminate. Due to the importance and the requirement of
all tasks to be able to communicate with it the Task Launcher stores
the Nameserver’s task id in a global variable that all tasks are able
reach.

The Nameserver takes in a null terminated character sequence of no
more than 8 characters (including the null terminator) and provides a
querying task with the task id that corresponds to that string. If a
new task tries to register the same name as another task that
registered with the name server then the lookup will now point to the
task that registered most recently. The internal storage of the server
allows for up to 32 different lookups to be registered with it.

The Nameserver stores all of the names into a single unsorted array
and all of the tasks that each name into a separate array such the the
index of both arrays correspond to the same element. Since the name
size is 8 characters or less the server does word comparison though
the name array until a match is found or all of the currently
registered names have been exhausted. The reason we used 8 as the size
is it allows for a sufficient large space of names and fits nicely
into two words so only two full comparisons need to be performed for
every name speeding up the liner search by a little while still
maintaining a simple approach for storage. Liner search was chosen due
to its simplicity and the Nameserver while important should only
require use during the initialization or when having a longer
operating time is of little consequence.

Another benefit of the method chosen for storage is that it makes reverse
lookup as easy as searching though the task list for the first occurrence
of a particular task id. The ability for the name server to also be able to
perform a reverse lookup is also greatly beneficial for many stages of our
debugging.

\subsection*{Mission Control}

Mission Control is the task that is set up to be in control of the current
state of the track on the train set. The first thing that Mission Control
does is send out a grouping of commands to reset all of the switches into a
state that is known by the OS. After the track has been reset successfully
it kicks off a task to act as a sensor poller to notify mission control when
one of the sensors on the track is hit so the track can update the GUI
accordingly.

The switches are placed into a null terminated circular buffer. The reason why
we chose to null terminate this particular buffer is that it made it easier to
print out the contents in a FILO order that works nicely when the queue is
also not completely full during the beginning of the application.

All of the turnout states are placed into a standard array that uses a mapping
function to zero index off the the switch numbers into a position that is
contiguous in respect to the other turnouts.

\subsection*{Terminal Server}

\subsection*{Train Server}

The train server works like a slightly simpler terminal server. When the train
server starts up the first thing it does is register its self with the
Nameserver. Next, it creates a task to act as a Notifier to get characters that
are sent by the train and another task that will take characters from the
server and forward them to the train server.

The train server uses 2 circular character buffers to store both the incoming
and outgoing characters while it waits for their appropriate tasks to pick them
up for processing. the Train server doesn't have logic to block tasks as the
number of commands that need to be sent to the train server at a time are
relatively small and trying to overflow the buffers would require trying to
make everything on the train set do opposing actions at the same time.

\subsubsection*{Train Write Carrier}

The train write character tries to grab up to 4 characters at a time from the
server and stream them one at a time over to the train set. Between every
\ttt{UART1\_SEND} event the carrier ensures that the clear to send line has
both gone down and up to ensure that the train set has had enough time to
process the previous character. This is done though both the \ttt{UART1\_CTS}
and \ttt{UART1\_DOWN} events.

\subsubsection*{Train Receive Notifier}


The terminal server serializes and manages all output to the terminal
serial line. It creates a send carrier and receive notifier task to
handle hardware events for sending and receiving data from UART2, the
serial connection to the terminal.

Output is normally buffered in the terminal server to avoid blocking
tasks that want to print to the screen. However, if the system is
under load and cannot pump characters onto the line fast enough, then
tasks will be blocked and queued in a circular buffer until their
output can be handled. This situation has only come up for under while
intentionally stressing the system.

To streamline the process of output, we make the observation that most
output is of strings of characters. We added the \ttt{Puts} system
call to optimize passing strings to the terminal server instead of
going through the \ttt{Putc} system call. Strings are fully formatted
on the calling task and the terminal server only handles output.

We use two buffers for output. We use a ping pong buffering strategy
between the terminal server and the send carrier. This simplifies
managing buffers between the two tasks and eliminates the need to make
an additional copy of the buffer.

All input is buffered in the terminal server. Once input comes in from
the receive notifier task, it is added to a circular buffer in the
terminal server. Tasks can get input using the \ttt{Getc} system
call. If no input is available then the task requesting input will be
blocked until data comes in.

However, only a single task can be blocked waiting for input. Our
system has checks to ensure that only one task is ever handling input
at a time. Our system is implemented so that only the task launcher
task will be processing input.

Since the system will need to process input character by character, we
did not bother to create an optimized code path for getting large
amounts of characters from the terminal server. However, our system
does make use of the UART FIFOs, so the receive notifier can cause
multiple characters to be buffered in the terminal server in a single
notification.

\subsection*{Task Launcher}

Handles the command prompt. Parses input. Sends commands off.

\subsection*{Train Station}

All train commands are buffered in a warehouse task called the train
station. The train station maintains a separate circular buffer of
commands for each train and instantiates a \ttt{train} task for each
possible train in the lab.

When a train task is ready to execute a new operation it will request
a job from the train station. If no jobs are available then the train
task will block until there are jobs available.

Buffering commands in a warehouse allows long running commands, such
as reversing the train, to not block the command prompt.


\subsection*{Train}

Train tasks anthropomorphize trains on the track. They will simply ask
for jobs from the train station warehouse task and be blocked until
there is a job to perform.

Jobs include changing speed, stopping, reversing, toggling lights, and
toggling the horn function. Implementing train control logic in this
way made handling train reverse operations trivial as a train task can
block without having to worry about blocking while waiting for the
train to stop without having to block the command prompt or any
other part of the system.

There are seven instances of the train task running by default, one
for each train in the lab. Each train task asks the train station
which train it represents at boot time and then registers the
name ``TRAINnn'' with the name server, where ``nn'' is the trains
number.

\subsection*{Idle Task}

The idle task is created at startup and runs at the lowest priority in
the system. The task constantly polls the 40-bit timer in the system
and checks the difference from the last time the timer was polled. If
the difference is greater than 2 ticks, then we know that the system
was interrupted performing real work between polling. In this case,
the difference is added to a counter for the total number of non-idle
time. This sum can be compared against the amount of time since the
system started to calculate the percentage of non-idle time. The
complement of this value is the idle time of the system.

This works because the system can poll the 40-bit timer in much less
than 2 ticks of the timer. However, if the idle task is interrupted by
hardware events it will take the system at least two context switches
before it can return to the idle task.

Measuring idle time in this way does not impact the scheduler or any
important component of the system. However, the accuracy of idle time
calculation decreases as the amount of non-idle time increases.

\subsubsection*{Idle UI}

A separate task, running at a higher priority, polls the idle time
counter once per second and calculates the idle time for the past
second. The counter is then reset and the idle time, as a percentage,
is then printed to the screen.

An indicator is printed after the idle time that will change every
time the idle value is printed. This is to indicate that the idle time
is being updated and the system has not hung.

An update time of one second was chosen because it needed to be stable
on screen long enough for a human to read the value. Though, we may
need to make it update more frequently in the future.

\section*{Terminal Commands}

A full list of the commands that the terminal supports.

\section*{Abortions And Assertions}

Consistency checks are peppered liberally throughout the code base
asserting that arguments and messages contain valid values. In the
kernel we also have several checks to make sure that task state is not
corrupted between system calls and hardware interrupts.

Corruption checks include checking that the task stack pointer is in a
valid range and that the program counter for the task is within the
text range for the kernel. If a corruption check fails then execution
of the system is aborted.

System calls will return an error code in almost all error cases,
allowing the calling task to handle the error. However, most tasks
will handle the error by aborting execution as system call errors are
not an expected behaviour for any task currently implemented.

Other errors in task land that we think that we can recover from are
usually handled by returning from the function with an error code
indicating the error. If the caller does not want to handle the error
then execution is aborted.

In general, the vast majority most consistency checks are in the form
of assertions and will cause execution to abort if the check fails.

\subsection*{Magic SysRq Key}

Borrowing the concept of the magic system request key from
Linux. Pressing the tilde (\ttt{`}) key at any point during runtime
will cause execution of the system to abort and print out the full
debug tableau.

The request key should respond no matter what state tasks are in as
long as the kernel itself has not become corrupt. The value of being
able to do this is an abortion will dump the debug tableau to the
terminal.

\subsection**{Debug Tableau}

As mentioned in the \ttt{Abort} system call documentation, when
execution is aborted the full state of the kernel is dumped to the
terminal. This information includes the full table of task
descriptors, the currently active task, and a listing of all tasks
waiting on hardware interrupts.

The task descriptor table will include every descriptor that has been
allocated, even if the descriptor is currently unused. Unused
descriptors are indicated by having a \ttt{-} value in the stack
pointer column.

To make mental reconstruction of the task state easier, we attempt to
replace all task identifiers with names by performing a reverse lookup
in the name server.

The table contains enough information to reconstruct what state each
task was in, including the stack pointer and program counter. The
table includes information about messaging, including which task was
blocked sending or waiting for a message. The scheduling priority
queues and message receive queues can be reconstructed from the table
with some effort. Information about which tasks were blocked on
particular hardware events, and the currently active task, are listed
above the table.

\section*{Known Bugs}

\begin{itemize}
\item The clock UI will stop updating at 100 minutes. However, the
  system time will continue to increment normally.
\item On rare occasion when trying to restart the OS, the clock will be in a
  bad state and cause the terminal UI to lock up. Restarting the
  application again will fix the issue. This issue has never
  manifested on the first run after the box has been reset, only
  during every other subsequent run.
\end{itemize}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
