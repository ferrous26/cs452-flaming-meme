\documentclass[pdftex,10pt,a4paper]{article}
\usepackage{../cs452}
\usepackage{verbatim}

\begin{document}

\kernelmake{4}

\section*{Overview}

This kernel development milestone adds the final set of kernel
features required for train control, namely serial I/O for both
the terminal and train controller. To demonstrate the finished kernel,
the M{\"a}rklin train set can be controlled by user input
to a terminal, and train track state is updated live on screen as the
trains travel around the track.

Commands issued into the terminal can manipulate trains and
switches, as well as monitor sensors. Trains can start, stop, toggle
lights and sound effects, and adjust speed. Switches can be toggled or
explicitly set to the straight or curved state.

The terminal displays information about current state
of the trains and switches, including a short history of the most
recent sensor activations. However, sensor activity cannot be
controlled by the user in any way.


\section*{Operation Instructions}

A pre-compiled kernel exists at
\ttt{/u/cs452/tftp/ARM/marada/k4.elf}, which can be loaded with
RedBoot using the following command:

\begin{center}
  \ttt{load -h 10.15.167.4 ARM/marada/k4.elf; go}
\end{center}

Notice that no offset should be specified for the load instruction.

The source code for the kernel exists in \ttt{/u3/marada/kernel4/},
and can be compiled with the following command chain:

\begin{center}
  \ttt{cd /u3/marada/kernel4 \&\& /u/cs444/bin/rake local}
\end{center}

Which will produce a \ttt{kernel.elf} file in the same directory as
the makefile, built in release mode with benchmarking enabled.

After Initialization the kernel will print
``\ttt{Welcome to Task Launcher}'' into the message logging area of
the screen and a command prompt will appear near the center of the
screen titled ``TERM> ''. At this point commands can be entered; keys
pressed should be echoed back at the command prompt in place of the
white cursor. Pressing the return key will confirm a command and cause
it to be processed. An online listing of the commands can be printed
by entering an empty command.

\subsection*{Submitted Files}
\begin{center}
\begin{tabular}{l|l}
  \bfseries File & \bfseries MD5 Hash
  \\\hline
  \csvreader[head to column names]{md5_info.csv}{}%
  {\\\file & \ttt{\hash}}%
\end{tabular}
\end{center}

\newpage
\part*{The Kernel}

The kernel is organized into four sections: task management,
scheduling tasks, system calls between tasks, and handling interrupt
driven hardware events.

\section*{Task Management}

Task metadata is stored in a static array of task descriptors. The
array holds 64 descriptors, allowing for up to 64 concurrent tasks to
run on our system. The constant \ttt{TASK\_MAX} is used to represent
the maximum number of concurrent tasks and is used by various other
components of the system to provide upper limits on data structures
and calculations.

Each task descriptor, defined as type \ttt{task}, tracks the following
task metadata:

\begin{description}
  \item[\ttt{int tid}] \hfill \\
    Task identifier. A globally unique identifier for the task. The
    identifier is stored as a signed integer as it is the simplest way
    to maintain consistency with the required API for the
    \ttt{Create} system call.

    The task identifier value is tied to the array index of the
    descriptor so that no two live tasks can have the same task
    identifier. When a task \ttt{Exit}s the descriptor will increment
    the \ttt{tid} by \ttt{TASK\_MAX} so that the next task to use the
    descriptor will not reuse a previously allocated task
    identifier. This scheme also allows us to calculate the index of
    the descriptor in a single instruction given the task's identifier.

    No safety is provided at the point where the task identifier
    values overflow, though we do not have any use cases which
    approach the possibility of causing an overflow.

  \item[\ttt{int p\_tid}] \hfill \\
    Parent task Identifier. A copy of the \ttt{tid} of the task which
    \ttt{Create}d the task occupying the descriptor. This value is a
    copy because the only time that the parent is guaranteed to be
    alive concurrently with the child is during the \ttt{Create} call
    that allocates the child task. The first task that is run by our
    kernel has a \ttt{p\_tid} value of its own \ttt{tid}---the task is
    effectively its own parent.

  \item[\ttt{int priority}] \hfill \\
    Priority level. Valid priority values are between $0$ and $31$,
    with $31$ being the highest priority and $0$ being the lowest
    priority. This range of priorities is efficiently represented by a
    single word bitmap during scheduling. Though this value
    does not require the full 32 bits of space provided by an integer,
    storing it as a full word sized value makes memory access easier
    (faster) for the hardware.

    We have defined constants for some priority levels, such as
    \ttt{TASK\_PRIORITY\_EMERGENCY} for level $31$ and
    \ttt{TASK\_PRIORITY\_MEDIUM} for level $15$.

  \item[\ttt{task* next}] \hfill \\
    Next task. This meaning of this value depends on the state of the
    task.

    During message passing the value is either a valid pointer
    to another task descriptor, \ttt{NULL}, or it contains a special
    value that indicates that the process is blocked.

    During all other times, the \ttt{next} pointer will contain a
    valid pointer to the next task of the same \ttt{priority} which is
    ready to run, or \ttt{NULL} if no such task exists.

  \item[\ttt{int* sp}] \hfill \\
    Task stack pointer. This value is \ttt{NULL} for descriptors which
    are not allocated to live tasks. When belonging to a live task,
    this value points to the top of the stack for the task at the
    point when the task execution was last interrupted. The top of the
    stack will conveniently also point to the trap frame for the task.
\end{description}

Task \ttt{state} is not explicitly stored in a task descriptor because
our kernel implementation only needs to know state in cases where the
\ttt{next} pointer or \ttt{sp} pointer will not be used. The
\ttt{next} pointer and \ttt{sp} pointer are overloaded to contain
\ttt{state} information in a way that has no performance overhead for
task scheduling and such that we only need to check exactly one of
those values when interested in a particular state.


\subsection*{Task Initialization}

Task creation and initialization is implemented in the
\ttt{task\_create} function.

To get a descriptor, we consume a value from the free list, which
is a circular buffer of descriptor indices. If the free list is empty
then no descriptors are available and an error code is returned
indicating the problem.

The descriptor will already have the correct \ttt{tid} value, either
set during system initialization or during task cleanup from the
previous user of the descriptor.

The \ttt{p\_tid} is set using the calling task's \ttt{tid}. The
\ttt{priority} value is set using an argument passed to the task
creation function which would normally map to the same value passed to
the \ttt{Create} system call.

The initial \ttt{sp} is then set and the default trap frame is
initialized in place on the task's stack. A task descriptor will
always be given the same initial stack pointer based on the index of
the descriptor.

The default trap frame sets the correct values for the program
counter, stack, and CPSR registers. Notably, the default link register
for a task is a pointer to the \ttt{Exit} system call, which ensures
that any task that does not explicitly call \ttt{Exit} instead of
simply \ttt{return}ing will still cleanup properly.

Finally, the task is given to the scheduler and scheduled to run.

\subsection*{Task Cleanup}

Task cleanup is implemented in the \ttt{task\_destroy} function.

A task is only cleaned up when it calls \ttt{Exit} and enters the
\ttt{ZOMBIE} state.

First, the \ttt{tid} of the descriptor is updated for the next
descriptor occupant. Then the \ttt{sp} is set to \ttt{NULL} to
indicate that the descriptor does not contain a live task.

Then the receive queue, used in message passing, must be flushed in
order to unblock any tasks that were trying to send a message to
now-zombified task.

Finally, the descriptor is added to the end of the free list so that
it can be allocated to a new task.


\section*{Scheduling}

Task scheduling is implemented in the \ttt{schedule\_schedule}
function.

The task scheduler is implemented as a bit field, which maps bits to
priority levels, and an array of \ttt{head} and \ttt{tail} pointers,
which are used for priority queues. The \ttt{next} pointer in the task
descriptors is also used for priority queues when needed.

When a task is scheduled, the scheduler is given a pointer to the
descriptor of the task to be scheduled. With the descriptor, the
\ttt{priority} level can be looked up, and the correct bit in the bit
field can be turned on to indicate a waiting job at that priority
level.

The correct pair of \ttt{head} and \ttt{tail} pointers are also looked
up using the \ttt{priority}. Using the value of the \ttt{tail}
pointer, the descriptor that is currently at the end of the list can
have its \ttt{next} pointer updated to point to the task being
scheduled. However, if there were no other task in the queue, then
the \ttt{head} pointer is set instead.

The \ttt{tail} pointer is always set to the task that was scheduled,
and the \ttt{next} pointer of the task being scheduled is always set
to \ttt{NULL} to indicate the end of the list.

By using the \ttt{next} index of the descriptor we saves the need to
allocate a full queue for each priority level. The \ttt{next} pointers
are also used for messaging queues to provide even more memory
savings.

\subsection*{Activation Selection}

Activation selection is implemented in the \ttt{scheduler\_get\_next}
function.

When the scheduler is asked to find the next task to be activated
during a system call or hardware interrupt, it will look for the
highest bit set in the bit field to select the priority queue and look
up the \ttt{head} pointer for that queue and then retrieve the correct
task descriptor.

Then, the \ttt{head} pointer is moved up using the \ttt{next} pointer
of the \ttt{head} task. If \ttt{next} was \ttt{NULL} then we know that
the queue is empty and the bit in the bit field should be turned off.

Finally, the \ttt{task\_active} global, which is a pointer to the
currently active task will be set to the selected task.

\section*{System Calls}

Just talk about the mechanism here, not any specific calls.

\subsection*{Core System Calls}

\subsubsection*{Create}
\subsubsection*{Pass}
\subsubsection*{Exit}
\subsubsection*{Abort}
\subsubsection*{Shutdown}

\subsection*{Message Passing}

\subsubsection*{Send}
\subsubsection*{Receive}
\subsubsection*{Reply}

\subsection*{Hardware Events}

\subsubsection*{AwaitEvent}

\subsection*{Message Passing Wrappers}

\subsubsection*{WhoIs}

And all the rest\ldots

\section*{Hardware Interrupts}

When we were designing our interrupt code we wanted the user space tasks to
have no direct interaction with the hardware. This philosophy was chosen since
it allowed us to make our user land easier to program in as well as allowing us
ensure that while interacting with the hardware an interrupt can not cause us
to perform an action that no longer reflects the state of the system.

\subsection*{Hardware Context Switch}

The Hardware interrupt code is built such that it encapsulates the software
interrupt. Since the software interrupt does not save scratch registers that
GCC will assume have been clobbered during a function execution the hardware
interrupt first saves those values and writes a flag into the position of the
old link register to let the exit code know that it needs to perform the extra
hardware interrupt exit code. This decision was made because it allows for us
to reuse all of our existing assembly and required for a very small amount of
change to the existing software interrupt entry code.

When we enter the kernel we will still enter into the function
\ttt{syscall\_handle()}. When we are servicing an interrupt the interrupt code
value will get set to 0. At this point we save save the \ttt{sp} like normal
and can perform the hardware interrupt similarly as if it were regular software
interrupt reusing the same schedule and exit path.

\subsection*{Vectored Interrupt Controller (\ttt{VIC})}

In order to handle interrupt priority and lookup we employ use of the vectored
interrupt controllers \ttt{VEC1} and \ttt{VEC2}. They are set up in a daisy
chained configuration such that the priority of any action on \ttt{VEC1} will
take priority over the actions on \ttt{VEC2}. This was the nature of hardware
and not a decision that was made by us. The decision to use the \ttt{VIC}s was
because it allowed us to greatly simply our interrupt handling code by
offloading a non-trivial amount of work onto the available hardware. As well
as the code simplification we also get increased performance as the work
required to get the appropriate interrupt service routine can be performed in
parallel to the program execution.

When we enter the kernel to handle a hardware interrupt we get the interrupt
service routine by reading the function pointer off of the \ttt{VEC1}
controller. After the routine had been run we write back to the controller to
notify it that the interrupt has been serviced and it can get the next
interrupt service routine.

The largest issue is that when interrupt states are toggled between being
enabled or disabled the interrupt controller can get out of sync and will try
to handle the newly disabled interrupt though the default interrupt routine.
We were able to work around this by using a void function that does nothing but
return allowing for us to retrieve the next interrupt service routine from the
\ttt{VIC}.

\subsection*{\ttt{UART1} Events}
To interact with \ttt{UART1} we enable and use a combination of 3 different
interrupts. The reason that we use 3 different interrupts instead of just the
\ttt{UART1} or interrupt is because with the use of the \ttt{VIC} we can make
assumptions of the state of \ttt{UART1} based on which ISR gets run simplifying
the \ttt{UART} handling code greatly.

The interrupts are:
\begin{description}
\item[Interrupt 23 - \ttt{UART1} Receive] \hfill \\
	This is the Interrupt that is enabled with the highest priority on
	\ttt{VEC1}. The reason for this is because it has the smallest window
	of time before the byte in the hold register is lost so we ensure that
	it will always be handled first.
\item[Interrupt 24 - \ttt{UART1} Send] \hfill \\
	This interrupt has the second lowest priority on \ttt{VIC1}. This is
	because most actions that are going to be sent to the train controller
	are time sensitive and should go out when possible. However, the
	interrupt its self does not require for us to handle it before some
	amount of time has passed.
\item[Interrupt 52 - \ttt{UART1} General] \hfill \\
	This Interrupt is an or of all of the interrupt cases of \ttt{UART2}.
	Since the \ttt{UART1} Send and Receive interrupts are of a higher
	priority if the ISR for this interrupt gets invoked then it should be
	because of a modem interrupt. This Interrupt is handled with the
	highest priority on \ttt{VEC2} due to the requirement that the events
	of this interrupt are handled as fast as possible in order to ensure
	that the \ttt{UART1} send carrier can maintain proper flow control
	with the train controller.
\end{description}

\subsection*{\ttt{UART2} Events}
Similarly, \ttt{UART2} uses a combination of 3 different interrupts:
\begin{description}
\item[Interrupt 25 - \ttt{UART2} Receive] \hfill \\
	This is the Interrupt that is enabled with the second highest priority
	on \ttt{VEC1}. The reason for this is because it is the only other
	interrupt inside of \ttt{VEC1} that if we do not service it in an
	applicible amount of time will also cause an error in the operation of
	the hardware. Due to the use of the FIFO we have quite a bit more time
	before bytes start getting lost.
\item[Interrupt 26 - \ttt{UART2} Send] \hfill \\
	This interrupt is enabled with the lowest priority inside of
	\ttt{VIC1}. The reason for this is because the terminal has a high
	enough bandwidth that the throughput to it only seems to be an issue
	in extrodinary circumstances and the interrupt its self is not time
	sensitive with regards to when it needs to be fulfilled.
\item[Interrupt 54 - \ttt{UART2} General] \hfill \\
	This Interrupt is an or of all of the interrupt cases of UART2.
	Since the \ttt{UART2} Send and Receive interrupts are of a higher
	priority if the ISR for this interrupt gets invoked then it should
	be because of a receive timeout interrupt. This interrupt is handled
	with the lowest priority of \ttt{VEC2} since if too many characters
	get into the receive queue then the \ttt{UART2} receive interrupt
	will then go off instead. With this guarantee we know that at this
	point it is not pertinent to clear the receive queue.
\end{description}


\subsection*{Clock Events}

\begin{description}
\item [Interrupt 51 - Clock Underflow] \hfill \\
	This interrupt is run at a priority between the 2 \ttt{UART} priorities.
	This decision was because the timer has a relatively large window of
	time before we need to reset the interrupt state.
\end{description}

The clock only has 1 event that is triggered every time that the clock
underflow interrupt is thrown. If there is no task to wake up then we know
that a clock tick will have been missed and will log a message with the system.


\section*{Tasks}

List all the different tasks we have, their purpose, and the priority
that they run at.

In the case of server tasks, we need to refer back to the wrapper
functions on how to interact.

\subsection*{Clock Server}

The clock server is implemented in two parts. The first part, the
\ttt{clock\_notifier}, notifies the second part, the
\ttt{clock\_server}, on clock ticks.

The \ttt{clock\_notifier} starts by finding out \ttt{WhoIs(``clock'')}
and caching that \ttt{tid} for later. Then it enters a \ttt{FOREVER}
loop where it \ttt{Await}s for \ttt{CLOCK\_TICK} events from hardware
and notifies the \ttt{clock\_server} with a \ttt{Send} message whenever it
is woken up.

The \ttt{clock\_server} starts by registering itself with the name
server under the name \ttt{clock}. It then \ttt{Create}s the
\ttt{clock\_notifier} task at a higher priority level. The
\ttt{clock\_notifier} is run at the second highest priority level so
that it can perform its work as quickly as possible. The highest
priority level is reserved for emergency tasks.

After setting everything up, the \ttt{clock\_server} enters a
\ttt{FOREVER} loop and begins \ttt{Receive}ing messages from client
tasks. The primary client task is the \ttt{clock\_notifier}, which
causes the server to increment the \ttt{time} and check if any tasks
waiting for \ttt{Delay}s need to be woken up.

The server keeps track of tasks that wish to be \ttt{Delay}ed in a
heap based priority queue. The queue is large enough to store the
maximum number of concurrent tasks for the system. A priority queue
allows us to check if any task needs to be woken up in constant
time. A heap implementation, combined with our hypothetical use case
for the clock server, also has the property that new \ttt{Delay}
requests can be inserted into the queue in near constant time in the
average case, or at least less than logarithmic time in many
cases. All other operations occur in logarithmic time, which we expect
to be sufficient for small and medium work loads.

We had considered an alternative implementation for the priority queue
based on sorted doubly linked lists, as the only non-constant time
operation would have been to add a new element to the list. However,
the performance did not appear to improve, possibly because of the
overhead of dealing with allocating nodes for the queue.

\subsection*{Name Server}

The Nameserver is the first task that is started up on the
initialization of task launcher. Since the Nameserver is pivotal to
the proper operation of the OS if something goes wrong with the
initialization of the Nameserver the task launcher will abort causing
the kernel to terminate. Due to the importance and the requirement of
all tasks to be able to communicate with it the Task Launcher stores
the Nameserverâ€™s task id in a global variable that all tasks are able
reach.

The Nameserver takes in a null terminated character sequence of no
more than 8 characters (including the null terminator) and provides a
querying task with the task id that corresponds to that string. If a
new task tries to register the same name as another task that
registered with the name server then the lookup will now point to the
task that registered most recently. The internal storage of the server
allows for up to 32 different lookups to be registered with it.

The Nameserver stores all of the names into a single unsorted array
and all of the tasks that each name into a separate array such the the
index of both arrays correspond to the same element. Since the name
size is 8 characters or less the server does word comparison though
the name array until a match is found or all of the currently
registered names have been exhausted. The reason we used 8 as the size
is it allows for a sufficient large space of names and fits nicely
into two words so only two full comparisons need to be performed for
every name speeding up the liner search by a little while still
maintaining a simple approach for storage. Liner search was chosen due
to its simplicity and the Nameserver while important should only
require use during the initialization or when having a longer
operating time is of little consequence.

Another benefit of the method chosen for storage is that it makes reverse
lookup as easy as searching though the task list for the first occurrence 
of a particular task id. The ability for the name server to also be able to
perform a reverse lookup is also greatly beneficial for many stages of our
debugging.

\subsection*{Mission Control}

\subsection*{Train Server}

\subsection{Terminal Server}


\section*{Terminal Commands}

A full list of the commands that the terminal supports.

\section*{Abortions And Assertions}

This is where we talk about how we choose to detect and handle errors.




\section*{Known Bugs}

\begin{itemize}
\item Clock UI stops updating at 100 minutes. Internal time continues to 
	increase.
\item On rare occasion when trying to restart the OS the clock will be in a
	bad state and cause the terminal ui to lock up. Restarting the
	application again will fix the issue, this Issue will also never
	manifest on the first run after the box has been reset.
\end{itemize}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
