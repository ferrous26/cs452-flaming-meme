\documentclass[pdftex,10pt,a4paper]{article}
\usepackage{../cs452}
\usepackage{verbatim}

\begin{document}

\kernelmake{4}

\section*{Overview}

This kernel development milestone adds the final set of kernel
features required for train control, namely serial I/O for both
the terminal and train controller. To demonstrate the finished kernel,
the M{\"a}rklin train set can be controlled by user input
to a terminal, and train track state is updated live on screen as the
trains travel around the track.

Commands issued into the terminal can manipulate trains and
switches, as well as monitor sensors. Trains can start, stop, toggle
lights and sound effects, and adjust speed. Switches can be toggled or
explicitly set to the straight or curved state.

The terminal displays information about current state
of the trains and switches, including a short history of the most
recent sensor activations. However, sensor activity cannot be
controlled by the user in any way.


\section*{Operation Instructions}

A pre-compiled kernel exists at
\ttt{/u/cs452/tftp/ARM/marada/k4.elf}, which can be loaded with
RedBoot using the following command:

\begin{center}
  \ttt{load -h 10.15.167.4 ARM/marada/k4.elf; go}
\end{center}

Notice that no offset should be specified for the load instruction.

The source code for the kernel exists in \ttt{/u3/marada/kernel4/},
and can be compiled with the following command chain:

\begin{center}
  \ttt{cd /u3/marada/kernel4 \&\& /u/cs444/bin/rake local}
\end{center}

Which will produce a \ttt{kernel.elf} file in the same directory as
the makefile, built in release mode with benchmarking enabled.

After Initialization the kernel will print
``\ttt{Welcome to Task Launcher}'' into the message logging area of
the screen and a command prompt will appear near the center of the
screen titled ``TERM> ''. At this point commands can be entered; keys
pressed should be echoed back at the command prompt in place of the
white cursor. Pressing the return key will confirm a command and cause
it to be processed. An online listing of the commands can be printed
by entering an empty command.

\subsection*{Submitted Files}
\begin{center}
\begin{tabular}{l|l}
  \bfseries File & \bfseries MD5 Hash
  \\\hline
  \csvreader[head to column names]{md5_info.csv}{}%
  {\\\file & \ttt{\hash}}%
\end{tabular}
\end{center}

\newpage
\part*{The Kernel}

The kernel is organized into four sections: task management,
scheduling tasks, system calls between tasks, and handling interrupt
driven hardware events.

\section*{Task Management}

Task metadata is stored in a static array of task descriptors. The
array holds 64 descriptors, allowing for up to 64 concurrent tasks to
run on our system. The constant \ttt{TASK\_MAX} is used to represent
the maximum number of concurrent tasks and is used by various other
components of the system to provide upper limits on data structures
and calculations.

Each task descriptor, defined as type \ttt{task}, tracks the following
task metadata:

\begin{description}
  \item[\ttt{int tid}] \hfill \\
    Task identifier. A globally unique identifier for the task. The
    identifier is stored as a signed integer as it is the simplest way
    to maintain consistency with the required API for the
    \ttt{Create} system call.

    The task identifier value is tied to the array index of the
    descriptor so that no two live tasks can have the same task
    identifier. When a task \ttt{Exit}s the descriptor will increment
    the \ttt{tid} by \ttt{TASK\_MAX} so that the next task to use the
    descriptor will not reuse a previously allocated task
    identifier. This scheme also allows us to calculate the index of
    the descriptor in a single instruction given the task's identifier.

    No safety is provided at the point where the task identifier
    values overflow, though we do not have any use cases which
    approach the possibility of causing an overflow.

  \item[\ttt{int p\_tid}] \hfill \\
    Parent task Identifier. A copy of the \ttt{tid} of the task which
    \ttt{Create}d the task occupying the descriptor. This value is a
    copy because the only time that the parent is guaranteed to be
    alive concurrently with the child is during the \ttt{Create} call
    that allocates the child task. The first task that is run by our
    kernel has a \ttt{p\_tid} value of its own \ttt{tid}---the task is
    effectively its own parent.

  \item[\ttt{int priority}] \hfill \\
    Priority level. Valid priority values are between $0$ and $31$,
    with $31$ being the highest priority and $0$ being the lowest
    priority. This range of priorities is efficiently represented by a
    single word bitmap during scheduling. Though this value
    does not require the full 32 bits of space provided by an integer,
    storing it as a full word sized value makes memory access easier
    (faster) for the hardware.

    We have defined constants for some priority levels, such as
    \ttt{TASK\_PRIORITY\_EMERGENCY} for level $31$ and
    \ttt{TASK\_PRIORITY\_MEDIUM} for level $15$.

  \item[\ttt{task* next}] \hfill \\
    Next task. This meaning of this value depends on the state of the
    task.

    During message passing the value is either a valid pointer
    to another task descriptor, \ttt{NULL}, or it contains a special
    value that indicates that the process is blocked.

    During all other times, the \ttt{next} pointer will contain a
    valid pointer to the next task of the same \ttt{priority} which is
    ready to run, or \ttt{NULL} if no such task exists.

  \item[\ttt{int* sp}] \hfill \\
    Task stack pointer. This value is \ttt{NULL} for descriptors which
    are not allocated to live tasks. When belonging to a live task,
    this value points to the top of the stack for the task at the
    point when the task execution was last interrupted. The top of the
    stack will conveniently also point to the trap frame for the task.
\end{description}

\newpage

Task \ttt{state} is not explicitly stored in a task descriptor because
our kernel implementation only needs to know state in cases where the
\ttt{next} pointer or \ttt{sp} pointer will not be used. The
\ttt{next} pointer and \ttt{sp} pointer are overloaded to contain
\ttt{state} information in a way that has no performance overhead for
task scheduling and such that we only need to check exactly one of
those values when interested in a particular state.


\subsection*{Task Initialization}

Task creation and initialization is implemented in the
\ttt{task\_create} function.

To get a descriptor, we consume a value from the free list, which
is a circular buffer of descriptor indices. If the free list is empty
then no descriptors are available and an error code is returned
indicating the problem.

The descriptor will already have the correct \ttt{tid} value, either
set during system initialization or during task cleanup from the
previous user of the descriptor.

The \ttt{p\_tid} is set using the calling task's \ttt{tid}. The
\ttt{priority} value is set using an argument passed to the task
creation function which would normally map to the same value passed to
the \ttt{Create} system call.

The initial \ttt{sp} is then set and the default trap frame is
initialized in place on the task's stack. A task descriptor will
always be given the same initial stack pointer based on the index of
the descriptor.

The default trap frame sets the correct values for the program
counter, stack, and CPSR registers. Notably, the default link register
for a task is a pointer to the \ttt{Exit} system call, which ensures
that any task that does not explicitly call \ttt{Exit} instead of
simply \ttt{return}ing will still cleanup properly.

Finally, the task is given to the scheduler and scheduled to run.

\subsection*{Task Cleanup}

Task cleanup is implemented in the \ttt{task\_destroy} function.

A task is only cleaned up when it calls \ttt{Exit} and enters the
\ttt{ZOMBIE} state.

First, the \ttt{tid} of the descriptor is updated for the next
descriptor occupant. Then the \ttt{sp} is set to \ttt{NULL} to
indicate that the descriptor does not contain a live task.

Then the receive queue, used in message passing, must be flushed in
order to unblock any tasks that were trying to send a message to
now-zombified task.

Finally, the descriptor is added to the end of the free list so that
it can be allocated to a new task.


\section*{Scheduling}

Task scheduling is implemented in the \ttt{schedule\_schedule}
function.

The task scheduler is implemented as a bit field, which maps bits to
priority levels, and an array of \ttt{head} and \ttt{tail} pointers,
which are used for priority queues. The \ttt{next} pointer in the task
descriptors is also used for priority queues when needed.

When a task is scheduled, the scheduler is given a pointer to the
descriptor of the task to be scheduled. With the descriptor, the
\ttt{priority} level can be looked up, and the correct bit in the bit
field can be turned on to indicate a waiting job at that priority
level.

The correct pair of \ttt{head} and \ttt{tail} pointers are also looked
up using the \ttt{priority}. Using the value of the \ttt{tail}
pointer, the descriptor that is currently at the end of the list can
have its \ttt{next} pointer updated to point to the task being
scheduled. However, if there were no other task in the queue, then
the \ttt{head} pointer is set instead.

The \ttt{tail} pointer is always set to the task that was scheduled,
and the \ttt{next} pointer of the task being scheduled is always set
to \ttt{NULL} to indicate the end of the list.

By using the \ttt{next} index of the descriptor we saves the need to
allocate a full queue for each priority level. The \ttt{next} pointers
are also used for messaging queues to provide even more memory
savings.

\subsection*{Activation Selection}

Activation selection is implemented in the \ttt{scheduler\_get\_next}
function.

When the scheduler is asked to find the next task to be activated
during a system call or hardware interrupt, it will look for the
highest bit set in the bit field to select the priority queue and look
up the \ttt{head} pointer for that queue and then retrieve the correct
task descriptor.

Then, the \ttt{head} pointer is moved up using the \ttt{next} pointer
of the \ttt{head} task. If \ttt{next} was \ttt{NULL} then we know that
the queue is empty and the bit in the bit field should be turned off.

Finally, the \ttt{task\_active} global, which is a pointer to the
currently active task will be set to the selected task.

\section*{System Calls}

System calls are implemented in the standard way using the \ttt{swi}
instruction. Arguments, including the system call type, are passed
into the kernel via a pointer to a request object. The request object
itself lives on the stack of the task making the system call.

After completing the system call request, the calling task is
rescheduled and the next task to be run is activated.

The software interrupt code path is implemented in a totally
non-reentrant way which requires that hardware interrupts be disabled
while in the kernel. However, this did allow us to make certain
optimizations that shorten the time spent in the kernel for system calls.

As a general pattern, we try to perform argument checking outside of
the kernel in order to reduce the time that is spent inside of the
kernel where hardware interrupts are ignored.

\subsection*{Software Context Switch}

Software system calls are invoked via the wrapper function
\ttt{\_syscall()}. The reason we use this is so we can ensure the the
parameters are placed in the correct registers when the \ttt{swi} call
is performed. Another advantage to this is that GCC will ensure that
registers \ttt{r0-r3} as well as the \ttt{ip} are required for any further
task execution will be backed up.

At the time when the user task is rescheduled, the return value from
the system call will be in \ttt{r0}, which follows the standard
function calling convention.

The immediate value associated with the \ttt{swi} instruction is not
used. Instead, we pass the system call number via \ttt{r0}. Additional
arguments for the system call are stored in a \ttt{kernel\_req}uest
structure on the task's stack and a pointer to the structure is left
in \ttt{r1}.

When the system call jump is performed the \ttt{spsr} and \ttt{lr} are
placed into \ttt{r3} and \ttt{r2} respectively. After this, all of the
registers are saved onto the task's stack so that they can be reloaded
when the task is rescheduled. At this point the kernel stack pointer
is reset and the function \ttt{syscall\_handle} is invoked. The first
argument will be the system call code, the second argument is the
pointer to the \ttt{kernel\_req}uest structure, and the third argument
is the stack pointer of active user task. This is all that is needed
by the kernel to service the user task and maintain all of the task's
state.

On kernel exit the \ttt{r0} register will be used by the kernel to write
back the return value of the system call, and \ttt{r2} and \ttt{r3}
registers will hold information required to restore the user back to
the state.

Since our kernel is not designed to be reentrant, we do not save
any of the kernel's working registers. Every time we enter the kernel from
an interrupt, just before the branch into \ttt{syscall\_handle}, the
kernel's \ttt{sp} is reset to the value \ttt{0x300000}, which is
reserved space for the kernel stack. This decision has also allowed us
to make \ttt{syscall\_handle} a naked function. Combined, these
optimizations eliminate relatively large amounts of redundant work
from context switching making system calls faster and reducing the
amount of time spent in the kernel.


\subsection*{Core System Calls}

\subsubsection*{int Create(int priority, void (*code)(void))}

\begin{description}
\item[\ttt{priority}] the priority level of the task to be created
\item[\ttt{code}] pointer to the function where the new task will begin
\end{description}

Create and schedule a new task.

Returns the \ttt{tid} of the created task if successful, otherwise
negative corresponding to one the following errors:

\begin{description}
\item[\ttt{NO\_DESCRIPTORS (-1)}] No task descriptors are available for
  allocation
\item[\ttt{INVALID\_PRIORITY (-2)}] Priority level is negative or greater
  than \ttt{TASK\_PRIORITY\_MAX}
\end{description}


\subsubsection*{int myTid(void)}

Fetch the task identifier for the calling task.

Returns the \ttt{tid} of the calling task. This system call cannot
fail (except for maybe if there is a catastrophic hardware failure).


\subsubsection*{int myParentTid(void)}

Fetch the task identifier for the calling task's parent task.

Returns the \ttt{p\_tid} of the calling task. This system call cannot fail.


\subsubsection*{int myPriority(void)}

Fetch the task priority for the calling task.

Returns the \ttt{priority} of the calling task. This system call cannot fail.


\subsubsection*{int ChangePriority(int priority)}

\begin{description}
\item[\ttt{priority}] the new priority level for the calling task
\end{description}

Change the priority level of the calling task.

Returns the \ttt{priority} of the calling task. This system call cannot fail.


\subsubsection*{void Pass(void)}

This system call effectively does nothing. However, it can be used to
give up execution to the next task to run.


\subsubsection*{void Exit(void)}

Cease execution of the calling task. This system call does not return
and the descriptor for the calling task is cleaned. This system call
cannot fail.


\subsubsection*{void Abort(char* file, unsigned int line, char* msg, \ldots)}

Cease execution of the entire system and print out diagnostic
information, including the format string given by \ttt{msg}.

This system call is not usually called directly, but is instead called
by an assertion failure. Output is generated in the kernel to avoid
being interrupted by hardware and output using busy-wait I/O.

In addition to source information and the format string, output will
include a full dump of the kernel state. Kernel state includes the
descriptor table and information about tasks waiting on
interrupts. The tableau can be used to reconstruct what was going on
inside of the kernel, and, to a limited extent, what was going on in
each running task.

This system call cannot fail (unless the failure also causes the
kernel to become severely corrupted).


\subsubsection*{void Shutdown(void)}

Cease execution of the system. Unlike the \ttt{Abort} system call,
this exit does not include any debugging information and is meant to
be used for cleanly shutting down the system.

This system call does not return and cannot fail.


\subsection*{Message Passing}

Message sending is implemented using messaging queues. The queue
implementation uses the same logic as that of the priority queues for
scheduling, but the producers and consumers of the queue are
different. The instance data for both queues have some overlap as they
share use of the \ttt{next} pointer in task descriptors for their
queueing.

When a message is sent using the \ttt{Send} system call, the
descriptor enters the receive queue for the receiving task. If the
receiving task was already waiting for the message, then it is given
the message right away and woken up.

When a task wants to receive a message it uses the \ttt{Receive}
system call to check if there are any messages in its receive
queue. If a message is available it consumes the message from the
queue and sets the message sender into the \ttt{RPLY\_BLOCED} state.
If no message is available then it itself as being blocked waiting for a
message by setting its own \ttt{next} pointer to \ttt{RECV\_BLOCKED}.

The \ttt{RPLY\_BLOCKED} state is marked using the \ttt{next} pointer
of the message sender. Only tasks in the \ttt{RPLY\_BLOCKED} state can
be replied to using the \ttt{Reply} system call.

After a message has been received and processed, the receiver can
reply using the \ttt{Reply} system call. The \ttt{Reply} call must be
to a task in the \ttt{RPLY\_BLOCKED} state or it will fail.

In this system, we do not need to know if a task is
\ttt{SEND\_BLOCKED}, but we can still determine if a task
\ttt{SEND\_BLOCKED}, for debugging, by searching through the receive
queues.

The \ttt{RECV\_BLOCKED} and \ttt{RPLY\_BLOCKED} state are represented
as pointers to low memory addresses. The low memory addresses are
guaranteed to be invalid pointers to data from the kernel. This makes
it easy and reliable to have assertions in the kernel regarding task
state. The small addresses also can be represented in any assembly
instruction immediate slot, so it is also efficient.

Message passing is implemented in a simple way with minimal
overhead. A 4 byte message can be sent and replied to with another 4
byte message in approximately $\sim6.0\mu$seconds.


\subsubsection*{int Send(int tid, char* msg, int msglen, char* reply, int replylen)}

\begin{description}
\item[\ttt{tid}] The task identifier of the receiver of the message
\item[\ttt{msg}] A pointer to the buffer containing the message
\item[\ttt{msglen}] The size, in bytes, of the buffer containing the message
\item[\ttt{reply}] A pointer to the buffer where the message reply can
  be placed
\item[\ttt{replylen}] The size, in bytes, of the \ttt{reply} buffer
\end{description}

Send a message to another task. Using this function, a task can send an
arbitrary message (possibly an empty message) to any task in the
system except for itself. The message can be any data structure, it
does not need to be a string.

The task will remain blocked until it receives a \ttt{Reply}. The
\ttt{reply} buffer will contain the reply when the call
returns. However, the \ttt{reply} might be empty and the return value
must be checked to know the size of the reply.

If a task sends a message to itself it will deadlock itself. In debug
builds of the kernel this case is asserted against. However, release
builds do not include this check as it is an incredibly unlikely case
should have been caught during testing with debug builds.

If the receiver never receives the message then the sender will block
until the receiver \ttt{Exit}s.

If the receiver does not provide a receive buffer at least
\ttt{msglen} bytes in size then the sender will be rejected with an
appropriate error code.

This function returns the actual size of the \ttt{reply} if the
message send was successful, otherwise a negative value is returned
that corresponds to one the following errors:

\begin{description}
\item[\ttt{IMPOSSIBLE\_TASK (-1)}] The \ttt{tid} argument was negative
  and therefore impossible
\item[\ttt{INVALID\_TASK (-2)}] The \ttt{tid} argument does not
  correspond to a task this is currently alive
\item[\ttt{INCOMPLETE (-3)}] The receiver of the message died before
  receiving the message
\item[\ttt{NOT\_ENUF\_MEMORY (-4)}] The receiver did not provide a
  receive buffer large enough for \ttt{msg}
\end{description}


\subsubsection*{int Receive(int* tid, char* msg, int msglen)}

\begin{description}
\item[\ttt{tid}] The task identifier of the sender of the message
\item[\ttt{msg}] A pointer to the buffer where to place the message
\item[\ttt{msglen}] The size, in bytes, of the buffer for the message
\end{description}

Receive a, possibly empty, message from another task. This function
will block until a message is received. The received message will be
placed into the \ttt{msg} buffer that was given and the task
identifier of the message sender will be copied into \ttt{tid}.

If the message is larger than \ttt{msglen} then the sender will be
refused an the receiver will continue to block waiting for a message.

This function returns the actual size of the \ttt{msg} if the
message send was successful. There are no error codes returned by this
function as error handling is forced onto the message sender.


\subsubsection*{int Reply(int tid, char* reply, int replylen)}

\begin{description}
\item[\ttt{tid}] The task identifier of the receiver of the message
\item[\ttt{reply}] A pointer to the buffer with the reply message
\item[\ttt{replylen}] The size, in bytes, of the \ttt{reply} buffer
\end{description}

Reply to a message sender. Using this function, a task can unblock a
task that is \ttt{RPLY\_BLOCKED} waiting for a message
reply. Technically, any task can reply to a message, it does not have
to be the receiver of the message. However, in our kernel message
passing has only ever been between two tasks.

As with \ttt{Send}, the reply can be any arbitrary message, even an
empty message is permitted.

The task will not get blocked trying to reply. If the reply message
cannot be copied into the replyee's buffer then an error code is
returned.

This function returns \ttt{OK} ($0$) to indicate that the reply was
successful and the replyee has been unblocked. If an error occurred,
then a negative value is returned that corresponds to one of the
following errors:

\begin{description}
\item[\ttt{IMPOSSIBLE\_TASK (-1)}] The \ttt{tid} argument was negative
  and therefore impossible
\item[\ttt{INVALID\_TASK (-2)}] The \ttt{tid} argument does not
  correspond to a task this is currently alive
\item[\ttt{INVALID\_RECVER (-3)}] The replyee is not \ttt{RPLY\_BLOCKED}
\item[\ttt{NOT\_ENUF\_MEMORY (-4)}] The replyee did not provide a
  receive buffer large enough for \ttt{msg}
\end{description}


\subsection*{Hardware Events}

\subsubsection*{AwaitEvent}

\subsection*{Message Passing Wrappers}

\subsubsection*{WhoIs}

And all the rest\ldots

\section*{Hardware Interrupts}

When we were designing our interrupt code we wanted the user space tasks to
have no direct interaction with the hardware. This philosophy was chosen since
it allowed us to make our user land easier to program in as well as allowing us
ensure that while interacting with the hardware an interrupt can not cause us
to perform an action that no longer reflects the state of the system.

\subsection*{Hardware Context Switch}

The Hardware interrupt code is built such that it encapsulates the software
interrupt. Since the software interrupt does not save scratch registers that
GCC will assume have been clobbered during a function execution the hardware
interrupt first saves those values and writes a flag into the position of the
old link register to let the exit code know that it needs to perform the extra
hardware interrupt exit code. This decision was made because it allows for us
to reuse all of our existing assembly and required for a very small amount of
change to the existing software interrupt entry code.

When we enter the kernel we will still enter into the function
\ttt{syscall\_handle()}. When we are servicing an interrupt the interrupt code
value will get set to 0. At this point we save save the \ttt{sp} like normal
and can perform the hardware interrupt similarly as if it were regular software
interrupt reusing the same schedule and exit path.

\subsection*{Vectored Interrupt Controller (\ttt{VIC})}

In order to handle interrupt priority and lookup we employ use of the vectored
interrupt controllers \ttt{VEC1} and \ttt{VEC2}. They are set up in a daisy
chained configuration such that the priority of any action on \ttt{VEC1} will
take priority over the actions on \ttt{VEC2}. This was the nature of hardware
and not a decision that was made by us. The decision to use the \ttt{VIC}s was
because it allowed us to greatly simply our interrupt handling code by
offloading a non-trivial amount of work onto the available hardware. As well
as the code simplification we also get increased performance as the work
required to get the appropriate interrupt service routine can be performed in
parallel to the program execution.

When we enter the kernel to handle a hardware interrupt we get the interrupt
service routine by reading the function pointer off of the \ttt{VEC1}
controller. After the routine had been run we write back to the controller to
notify it that the interrupt has been serviced and it can get the next
interrupt service routine.

The largest issue is that when interrupt states are toggled between being
enabled or disabled the interrupt controller can get out of sync and will try
to handle the newly disabled interrupt though the default interrupt routine.
We were able to work around this by using a void function that does nothing but
return allowing for us to retrieve the next interrupt service routine from the
\ttt{VIC}.

\subsection*{\ttt{UART1} Events}
To interact with \ttt{UART1} we enable and use a combination of 3 different
interrupts. The reason that we use 3 different interrupts instead of just the
\ttt{UART1} or interrupt is because with the use of the \ttt{VIC} we can make
assumptions of the state of \ttt{UART1} based on which ISR gets run simplifying
the \ttt{UART} handling code greatly.

The interrupts are:
\begin{description}
\item[Interrupt 23 - \ttt{UART1} Receive] \hfill \\
	This is the Interrupt that is enabled with the highest priority on
	\ttt{VEC1}. The reason for this is because it has the smallest window
	of time before the byte in the hold register is lost so we ensure that
	it will always be handled first.
\item[Interrupt 24 - \ttt{UART1} Send] \hfill \\
	This interrupt has the second lowest priority on \ttt{VIC1}. This is
	because most actions that are going to be sent to the train controller
	are time sensitive and should go out when possible. However, the
	interrupt its self does not require for us to handle it before some
	amount of time has passed.
\item[Interrupt 52 - \ttt{UART1} General] \hfill \\
	This Interrupt is an or of all of the interrupt cases of \ttt{UART2}.
	Since the \ttt{UART1} Send and Receive interrupts are of a higher
	priority if the ISR for this interrupt gets invoked then it should be
	because of a modem interrupt. This Interrupt is handled with the
	highest priority on \ttt{VEC2} due to the requirement that the events
	of this interrupt are handled as fast as possible in order to ensure
	that the \ttt{UART1} send carrier can maintain proper flow control
	with the train controller.
\end{description}

\subsection*{\ttt{UART2} Events}
Similarly, \ttt{UART2} uses a combination of 3 different interrupts:
\begin{description}
\item[Interrupt 25 - \ttt{UART2} Receive] \hfill \\
	This is the Interrupt that is enabled with the second highest priority
	on \ttt{VEC1}. The reason for this is because it is the only other
	interrupt inside of \ttt{VEC1} that if we do not service it in an
	applicable amount of time will also cause an error in the operation of
	the hardware. Due to the use of the FIFO we have quite a bit more time
	before bytes start getting lost.
\item[Interrupt 26 - \ttt{UART2} Send] \hfill \\
	This interrupt is enabled with the lowest priority inside of
	\ttt{VIC1}. The reason for this is because the terminal has a high
	enough bandwidth that the throughput to it only seems to be an issue
	in extraordinary circumstances and the interrupt its self is not time
	sensitive with regards to when it needs to be fulfilled.
\item[Interrupt 54 - \ttt{UART2} General] \hfill \\
	This Interrupt is an or of all of the interrupt cases of UART2.
	Since the \ttt{UART2} Send and Receive interrupts are of a higher
	priority if the ISR for this interrupt gets invoked then it should
	be because of a receive timeout interrupt. This interrupt is handled
	with the lowest priority of \ttt{VEC2} since if too many characters
	get into the receive queue then the \ttt{UART2} receive interrupt
	will then go off instead. With this guarantee we know that at this
	point it is not pertinent to clear the receive queue.
\end{description}

These three interrupts can be combined into 2 different events that are used to
perfrom all of the tasks required by the terminal server. Since we use the FIFO
queue on the \ttt{UART} we need to ensure that when less than 8 characters are
are placed into the buffer that we will still make them available to the system.

A \ttt{UART2\_RECV} event will wake up a task after data has been picked up
from \ttt{UART2}. The data is placed into the buffer provided by the task and
returns to number of character that have been placed into the tasks buffer. If
the \ttt{UART} has more characters than buffer space is available then the event
will only fill the buffer hoping that a task will wait on the event again
before the FIFO overflows.

A \ttt{UART2\_SEND} event takes a buffer from a task and will place up to 8
characters into the \ttt{UART2} FIFO queue. If less characters are available then
all of the characters will get placed into the queue. This event will return
the number of characters it was able to successfully place into the \ttt{UART}
before it rescheduled the user task.

\subsection*{Clock Events}

The clock only requires the use of one interrupt:
\begin{description}
\item [Interrupt 51 - Clock Underflow] \hfill \\
	This interrupt is run at a priority between the 2 \ttt{UART} priorities.
	This decision was because the timer has a relatively large window of
	time before we need to reset the interrupt state.
\end{description}

The clock only has one event that is triggered every time that the clock
underflow interrupt is thrown. If there is no task to wake up then we know
that a clock tick will have been missed and will log a message with the system.


\section*{Tasks}

List all the different tasks we have, their purpose, and the priority
that they run at.

In the case of server tasks, we need to refer back to the wrapper
functions on how to interact.

\subsection*{Clock Server}

The clock server is implemented in two parts. The first part, the
\ttt{clock\_notifier}, notifies the second part, the
\ttt{clock\_server}, on clock ticks.

The \ttt{clock\_notifier} starts by finding out \ttt{WhoIs(``clock'')}
and caching that \ttt{tid} for later. Then it enters a \ttt{FOREVER}
loop where it \ttt{Await}s for \ttt{CLOCK\_TICK} events from hardware
and notifies the \ttt{clock\_server} with a \ttt{Send} message whenever it
is woken up.

The \ttt{clock\_server} starts by registering itself with the name
server under the name \ttt{clock}. It then \ttt{Create}s the
\ttt{clock\_notifier} task at a higher priority level. The
\ttt{clock\_notifier} is run at the second highest priority level so
that it can perform its work as quickly as possible. The highest
priority level is reserved for emergency tasks.

After setting everything up, the \ttt{clock\_server} enters a
\ttt{FOREVER} loop and begins \ttt{Receive}ing messages from client
tasks. The primary client task is the \ttt{clock\_notifier}, which
causes the server to increment the \ttt{time} and check if any tasks
waiting for \ttt{Delay}s need to be woken up.

The server keeps track of tasks that wish to be \ttt{Delay}ed in a
heap based priority queue. The queue is large enough to store the
maximum number of concurrent tasks for the system. A priority queue
allows us to check if any task needs to be woken up in constant
time. A heap implementation, combined with our hypothetical use case
for the clock server, also has the property that new \ttt{Delay}
requests can be inserted into the queue in near constant time in the
average case, or at least less than logarithmic time in many
cases. All other operations occur in logarithmic time, which we expect
to be sufficient for small and medium work loads.

We had considered an alternative implementation for the priority queue
based on sorted doubly linked lists, as the only non-constant time
operation would have been to add a new element to the list. However,
the performance did not appear to improve, possibly because of the
overhead of dealing with allocating nodes for the queue.

\subsection*{Name Server}

The Nameserver is the first task that is started up on the
initialization of task launcher. Since the Nameserver is pivotal to
the proper operation of the OS if something goes wrong with the
initialization of the Nameserver the task launcher will abort causing
the kernel to terminate. Due to the importance and the requirement of
all tasks to be able to communicate with it the Task Launcher stores
the Nameserverâ€™s task id in a global variable that all tasks are able
reach.

The Nameserver takes in a null terminated character sequence of no
more than 8 characters (including the null terminator) and provides a
querying task with the task id that corresponds to that string. If a
new task tries to register the same name as another task that
registered with the name server then the lookup will now point to the
task that registered most recently. The internal storage of the server
allows for up to 32 different lookups to be registered with it.

The Nameserver stores all of the names into a single unsorted array
and all of the tasks that each name into a separate array such the the
index of both arrays correspond to the same element. Since the name
size is 8 characters or less the server does word comparison though
the name array until a match is found or all of the currently
registered names have been exhausted. The reason we used 8 as the size
is it allows for a sufficient large space of names and fits nicely
into two words so only two full comparisons need to be performed for
every name speeding up the liner search by a little while still
maintaining a simple approach for storage. Liner search was chosen due
to its simplicity and the Nameserver while important should only
require use during the initialization or when having a longer
operating time is of little consequence.

Another benefit of the method chosen for storage is that it makes reverse
lookup as easy as searching though the task list for the first occurrence
of a particular task id. The ability for the name server to also be able to
perform a reverse lookup is also greatly beneficial for many stages of our
debugging.

\subsection*{Mission Control}

\subsection*{Train Server}

\subsection*{Terminal Server}


\section*{Terminal Commands}

A full list of the commands that the terminal supports.

\section*{Abortions And Assertions}

This is where we talk about how we choose to detect and handle errors.


\section*{Known Bugs}

\begin{itemize}
\item Clock UI stops updating at 100 minutes. Internal time continues to
	increase.
\item On rare occasion when trying to restart the OS the clock will be in a
	bad state and cause the terminal UI to lock up. Restarting the
	application again will fix the issue, this issue will also never
	manifest on the first run after the box has been reset.
\end{itemize}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
