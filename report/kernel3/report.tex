\documentclass[pdftex,10pt,a4paper]{article}
\usepackage{../cs452}

\begin{document}

\kernelmake{3}

\section*{Overview}

This kernel development milestone adds handling for hardware
interrupts, a clock server, and system calls for event handling and
communication with the clock server. This milestone also includes a
task that demonstrates the clock servers abilities.

\section*{Operation Instructions}

A pre-compiled kernel exists at
\ttt{/u/cs452/tftp/ARM/marada/k3.elf}, which can be loaded with
RedBoot using the following command:

\begin{center}
  \ttt{load -h 10.15.167.4 ARM/marada/k3.elf; go}
\end{center}

Notice that no offset should be specified for the load instruction.

The source code for the kernel exists in \ttt{/u3/marada/kernel2/},
and can be compiled with the following command chain:

\begin{center}
  \ttt{cd /u3/marada/kernel2 \&\& source /u3/marada/.cs452 \&\& make clean \&\& make}
\end{center}

Which will produce a \ttt{kernel.elf} file in the same directory as
the makefile built in debug mode with minimal optimizations enabled.

After Initialization the kernel will respond with the message ``\ttt{Welcome to
Task Launcher}''.
At this point the key \ttt{1} will start the clock server
demonstration. The \ttt{h} key will print a list of other commands
which are available to demonstrate features added in this milestone.

\subsection*{Submitted Files}
\begin{center}
\begin{tabular}{l|l}
  \bfseries File & \bfseries MD5 Hash
  \\\hline
  \csvreader[head to column names]{md5_info.csv}{}%
  {\\\file & \ttt{\hash}}%
\end{tabular}
\end{center}

\newpage
\section*{Hardware Interrupts}

\subsection*{Initalization}

The first operation that we perform is to turn off all of the interrupts 
this is because we don't know what state the system may be at right now so we
want to make sure that it is in one that we can successfully initialize to a
proper one. Next we turn on the interrupts that we have explicit handling for.

Last we set up the vectored interrupt controller such that the clock is on the
highest priority of \ttt{VEC2} since in this stage it is the only interrupt useful to
the operation of the kernel. Interrupts \ttt{0}, \ttt{1}, \ttt{63{ are all also given
a basic handling function which is also addressed in the corresponding \ttt{VEC1} or 
\ttt{VEC2} controller. This was done to make the code to get the currently triggered
interrupt much cleaner since we have all handing now done for us by the hardware.

\subsection*{Context Switch}

Hardware interrupts are handled in a similar fashion as a software interrupt.
Since the software interrupt makes a couple assumptions about which registers
will be saved going in to them the hardware interrupt code first sets things up
so it can drop into the software interrupt code gracefully.

On exit the same kernel exit code is run, however the hardware interrupt writes
\ttt{0} into the link register which causes the exit code to also load the extra
state that the hardware interrupt had saved previously before returning to the
code before the interrupt was triggered. 

The reason we used the existing code path is two-fold: first, using existing
code compresses code size allowing for better caching performance. Second,
having a single entry point allows us to much better track all entry and exit
from the kernel.

\subsection*{Handling}

The Hardware context switch enters on \ttt{syscall\_handle()} just like the
software context switch but with a handling code of 0. At this point the same
software interrupt code is run until it sees that a code 0 has been passed in
and moves to the specialized interrupt handling section.

At this point we get the address to the handling function from the \ttt{VEC} 
controller. After we have gotten the handler for current interrupt we just 
run it and then write back to the \ttt{VEC1} controller to notify it that the 
current interrupt has been handled. Finally, the interrupted task reschedules 
its self.

Afterwords we drop into the same scheduling code path that is run at the end of
any software interrupt and the kernel exits normally.

\subsection*{AwaitEvent()}

The signature we chose for events is
\ttt{int AwaitEvent(int eventid, char* event, int eventlen)}. We chose
this in anticipation of handling UART FIFO I/O in the next
milestone. Using this API, the returned \ttt{int} will be \ttt{0} if
the event occurred normally and volatile data is now in the
\ttt{event} buffer that was given in the system call. The return value
will be \ttt{INVALID\_EVENT} (\ttt{-1}) if the given \ttt{eventid} is
not valid. No other return values are implemented for this milestone.

The contents of the \ttt{event} buffer after \ttt{AwaitEvent} returns
will depend on the event. For this milestone, only the
\ttt{CLOCK\_TICK} event is implemented, and will return once the clock
timer has signalled the CPU. The \ttt{CLOCK\_TICK} event does not put
any data into the \ttt{event} buffer as there is no meaningful data to
include. The \ttt{clock\_notifier} task, which is the notifier for the
\ttt{clock\_server} task, calls \ttt{AwaitEvent} as
\ttt{AwaitEvent(CLOCK\_TICK, NULL, 0)}.

Once the UART I/O is implemented, the \ttt{event} buffer will either
contain some bytes to be written, or be filled with bytes from the
UART. In the case of incoming data from a UART, the return from
\ttt{AwaitEvent} will be a positive value indicating how many bytes
were put into the \ttt{event} buffer.

\section*{Clock Server}

The clock server is implemented in two parts. The first part, the
\ttt{clock\_notifier}, notifies the second part, the
\ttt{clock\_server}, on clock ticks.

The \ttt{clock\_notifier} starts by finding out \ttt{WhoIs(``clock'')}
and caching that \ttt{tid} for later. Then it enters a \ttt{FOREVER}
loop where it \ttt{Await}s for \ttt{CLOCK\_TICK} events from hardware
and notifies the \ttt{clock\_server} with a \ttt{Send} message whenever it
is woken up.

The \ttt{clock\_server} starts by registering itself with the name
server under the name \ttt{clock}. It then \ttt{Create}s the
\ttt{clock\_notifier} task at a higher priority level. The
\ttt{clock\_notifier} is run at the second highest priority level so
that it can perform its work as quickly as possible. The highest
priority level is reserved for emergency tasks.

After setting everything up, the \ttt{clock\_server} enters a
\ttt{FOREVER} loop and begins \ttt{Receive}ing messages from client
tasks. The primary cilent task is the \ttt{clock\_notifier}, which
causes the server to increment the \ttt{time} and check if any tasks
waiting for \ttt{Delay}s need to be woken up.

The server keeps track of tasks that wish to be \ttt{Delay}ed in a
heap based priority queue. A priority queue allows us to check if any
task needs to be woken up in constant time. A heap implementation,
combined with our hypothetical use case for the clock server, also has
the property that new \ttt{Delay} requests can be inserted into the
queue in near constant time in the average case, or at least less than
logarithmic time in many cases.

\subsection*{Delay}

Negative and zero time delay values are checked by the system call
before context switching into the kernel. This has the added property
that a negative or zero delay request will not be forced to the back
of the scheduling queue, and will continue executing.

\subsection*{DelayUntil}

Unlike \ttt{Delay}, this call cannot fully validate its argument until
the message is received by the \ttt{clock\_server}. However, the
trivial checks for negative/zero values are still done in the client
task.

\subsection*{Time}

Simply copy the value from the server via a message pass.

\section*{Idle Task}

The \ttt{idle} task exists to ensure that there is always some task
ready to be activated by the scheduler. Even if all other live tasks
are blocked, the \ttt{idle} will still be able to run and waste CPU
cycles until a hardware interrupt occurs to wake up one of the other
tasks.

The \ttt{idle} task runs at the lowest possible priority. It is the
only task that runs at that priority to ensure it only runs when it
must. The current implementation will simply spin in a \ttt{FOREVER}
loop incrementing a counter.


\section*{Kernel Changes}

Fixed bugs related to task descriptor reuse.

Zero length messages can now be sent successfully and down a more
optimal path.

Cache pinning.

More tuning to the system call code path. We were able to convince the
compiler to not generate several redundant load and store operations on
the critical path for all system calls. Our message loop time is now
$\sim6.5\mu$seconds when clock interrupts are enabled.


\section*{Demonstration Output}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
