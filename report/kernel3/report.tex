\documentclass[pdftex,10pt,a4paper]{article}
\usepackage{../cs452}
\usepackage{verbatim}

\begin{document}

\kernelmake{3}

\section*{Overview}

This kernel development milestone adds handling for hardware
interrupts, a clock server, and system calls for event handling and
communication with the clock server. This milestone also includes a
task that demonstrates the clock servers abilities.

\section*{Operation Instructions}

A pre-compiled kernel exists at
\ttt{/u/cs452/tftp/ARM/marada/k3.elf}, which can be loaded with
RedBoot using the following command:

\begin{center}
  \ttt{load -h 10.15.167.4 ARM/marada/k3.elf; go}
\end{center}

Notice that no offset should be specified for the load instruction.

The source code for the kernel exists in \ttt{/u3/marada/kernel3/},
and can be compiled with the following command chain:

\begin{center}
  \ttt{cd /u3/marada/kernel3 \&\& /u/cs444/bin/rake local}
\end{center}

Which will produce a \ttt{kernel.elf} file in the same directory as
the makefile, built in release mode with benchmarking enabled.

After Initialization the kernel will respond with the message
``\ttt{Welcome to Task Launcher}''.
At this point the key \ttt{1} will start the clock server
demonstration. The \ttt{h} key will print a list of other commands
which are available to demonstrate features added in this milestone.

\subsection*{Submitted Files}
\begin{center}
\begin{tabular}{l|l}
  \bfseries File & \bfseries MD5 Hash
  \\\hline
  \csvreader[head to column names]{md5_info.csv}{}%
  {\\\file & \ttt{\hash}}%
\end{tabular}
\end{center}

\newpage
\section*{Hardware Interrupts}

\subsection*{Initialization}

The first operation that we perform is to mask all interrupts in the
interrupt controller. This is because we do not know what the state of
the system may be when the kernel boots and it is easiest to just
disable everything than to poll for the current state.

Next we unmask the interrupts that we are able to handle. At this
point the only interrupt that we can handle is from timer 3, being
used as the clock.

Finally, we setup the vectored interrupt controller such that the
clock is on the highest priority of \ttt{VEC2}. We chose the highest
priority since at this stage the clock is the only interrupt that is
useful to the operation of our system. Interrupts \ttt{0}, \ttt{1},
\ttt{63} are all also given a basic handling function which is also
addressed in the corresponding \ttt{VEC1} or \ttt{VEC2}
controller. Use of the vectored interrupt controller for handling
interrupt priority was done because it allowed the amount of code we
need to maintain smaller and cleaner, and allows the kernel to handle
interrupts faster.

\subsection*{Context Switch}

Hardware interrupts are handled in a similar fashion as a software
interrupt. Since the software interrupt makes a couple assumptions
about which registers will be saved, the hardware interrupt first sets
things up so that we can drop into the software interrupt code
gracefully.

When exiting the kernel from an IRQ event, we reuse the same kernel
exit code from software interrupts. However the hardware interrupt
writes \ttt{0} into the link register as part of the kernel entry
code, which causes the exit code to also load the extra state that the
hardware interrupt kernel entry had saved previously before returning
to the code right before the interrupt was triggered.

The reason we used the existing code path is two-fold: first, using
existing code compresses code size allowing for better caching
performance. Second, having a single entry point allows us to much
better track all entry and exit from the kernel.

\subsection*{Handling}

The Hardware context switch enters on \ttt{syscall\_handle()} just
like the software context switch but with a handling code of \ttt{0}. At
this point the same software interrupt code is run until it sees that
a code \ttt{0} has been passed in and moves to the specialized
interrupt handling section.

At this point we get the address to the handling function from the
\ttt{VEC}. After we have gotten the \ttt{handler} for current
interrupt we just call the \ttt{handler} and then write back to the
\ttt{VEC1} controller to notify it that the current interrupt has been
handled. Finally, the interrupted task reschedules its self.

Afterwords we drop into the same scheduling code path that is run at
the end of any software interrupt and the kernel exits normally.

\subsection*{AwaitEvent()}

The signature we chose for events is
\ttt{int AwaitEvent(int eventid, char* event, int eventlen)}. We chose
this in anticipation of handling UART FIFO I/O in the next
milestone. Using this API, the returned \ttt{int} will be \ttt{0} if
the event occurred normally and volatile data is now in the
\ttt{event} buffer that was given in the system call. The return value
will be \ttt{INVALID\_EVENT} (\ttt{-1}) if the given \ttt{eventid} is
not valid. No other return values are implemented for this milestone.

The contents of the \ttt{event} buffer after \ttt{AwaitEvent} returns
will depend on the event. For this milestone, only the
\ttt{CLOCK\_TICK} event is implemented, and will return once the clock
timer has signalled the CPU. The \ttt{CLOCK\_TICK} event does not put
any data into the \ttt{event} buffer as there is no meaningful data to
include. The \ttt{clock\_notifier} task, which is the notifier for the
\ttt{clock\_server} task, calls \ttt{AwaitEvent} as
\ttt{AwaitEvent(CLOCK\_TICK, NULL, 0)} and expect no data to be
returned.

Once the UART I/O is implemented, the \ttt{event} buffer will either
contain some bytes to be written, or be filled with bytes from the
UART. In the case of incoming data from a UART, the return from
\ttt{AwaitEvent} will be a positive value indicating how many bytes
were put into the \ttt{event} buffer.

\section*{Clock Server}

The clock server is implemented in two parts. The first part, the
\ttt{clock\_notifier}, notifies the second part, the
\ttt{clock\_server}, on clock ticks.

The \ttt{clock\_notifier} starts by finding out \ttt{WhoIs(``clock'')}
and caching that \ttt{tid} for later. Then it enters a \ttt{FOREVER}
loop where it \ttt{Await}s for \ttt{CLOCK\_TICK} events from hardware
and notifies the \ttt{clock\_server} with a \ttt{Send} message whenever it
is woken up.

The \ttt{clock\_server} starts by registering itself with the name
server under the name \ttt{clock}. It then \ttt{Create}s the
\ttt{clock\_notifier} task at a higher priority level. The
\ttt{clock\_notifier} is run at the second highest priority level so
that it can perform its work as quickly as possible. The highest
priority level is reserved for emergency tasks.

After setting everything up, the \ttt{clock\_server} enters a
\ttt{FOREVER} loop and begins \ttt{Receive}ing messages from client
tasks. The primary client task is the \ttt{clock\_notifier}, which
causes the server to increment the \ttt{time} and check if any tasks
waiting for \ttt{Delay}s need to be woken up.

The server keeps track of tasks that wish to be \ttt{Delay}ed in a
heap based priority queue. The queue is large enough to store the
maximum number of concurrent tasks for the system. A priority queue
allows us to check if any task needs to be woken up in constant
time. A heap implementation, combined with our hypothetical use case
for the clock server, also has the property that new \ttt{Delay}
requests can be inserted into the queue in near constant time in the
average case, or at least less than logarithmic time in many
cases. All other operations occur in logarithmic time, which we expect
to be sufficient for small and medium work loads.

We had considered an alternative implementation for the priority queue
based on sorted doubly linked lists, as the only non-constant time
operation would have been to add a new element to the list. However,
the performance did not appear to improve, possibly because of the
overhead of dealing with allocating nodes for the queue.


\subsection*{int Delay(int ticks)}

Negative and zero time delay values for \ttt{ticks} are checked by the
system call before context switching into the kernel. This has the
added property that a negative or zero delay request will not be
forced to the back of the scheduling queue, and will continue
executing.

A successful \ttt{Delay} will delay the task for at least \ttt{ticks}
clock ticks. A failure in the message sending will cause a negative
integer value to be returned from \ttt{Delay} which matches the error
codes for \ttt{Send}.

\subsection*{int DelayUntil(int ticks)}

Unlike \ttt{Delay}, this call cannot fully validate its argument until
the message is received by the \ttt{clock\_server}. However, the
trivial checks for negative/zero values are still done in the client
task.

If the clock server receives the message and finds out that the task
wants to be woken up at a time before the current time, then the task
is replied to immediately without entering the queue.

In error cases, \ttt{Delay} uses the same set of error codes as
\ttt{Delay}.

\subsection*{int Time()}

\ttt{Time} simply causes the clock server to reply to the sender with
a copy of its time. This operation does not block in any way other
than what is cause by message sending.

In error cases, \ttt{Time} uses the same set of error codes as
\ttt{Delay}.


\section*{Idle Task}

The \ttt{idle} task exists to ensure that there is always some task
ready to be activated by the scheduler. Even if all other live tasks
are blocked, the \ttt{idle} will still be able to run and waste CPU
cycles until a hardware interrupt occurs to wake up one of the other
tasks.

The \ttt{idle} task runs at the lowest possible priority. It is the
only task that runs at that priority to ensure it only runs when it
must. The current implementation will simply spin in a \ttt{FOREVER}
loop incrementing a counter.

The idle task allows us to make some assumptions in the kernel
scheduling, which further speed up system calls and hardware
interrupts.

For the next kernel milestone, we plan to track how much time is spent
in the idle task and have the kernel task report that value to some
sort of UI task that can print the value to the screen. We are
purposely avoiding UI widgets until we have serial I/O being driven by
interrupts.

\section*{Kernel Changes}

During development of this milestone we found two bugs related to task
descriptor reuse.

The first bug would only manifest once task descriptors had been
reused an the \ttt{tid} of the process no longer matched the index
into the descriptor table. This would cause memory to get into an
inconsistent state. The fix was simply to calculate the index from the
\ttt{tid} as we do elsewhere.

The second bug was related to error handling for message passing. Our
initialization of descriptors was lazy and descriptors would not be
initialized until they were needed. However, an invalid \ttt{tid} for
a message receiver would cause an uninitialized descriptor to be
accessed in such a way that it would cause random memory to be
written. We now initialize the entire descriptor array during
initialization. And, as an extra paranoid step, we \ttt{memset} the
entire \ttt{.bss} section before any initialization.

We also added the ability to send zero length messages between
tasks. This feature is used by the clock server to avoid sending junk
messages.

We have implemented cache pinning for the instruction and data
caches. Code and data which we believe will be used frequently is now
locked into the cache during initialization.

More tuning to the system call code path was done for this
milestone. We were able to convince the compiler to not generate
several redundant load and store operations on the critical path for
all system calls. Our message loop time is now $\sim6.5\mu$seconds
when clock interrupts are enabled.

We added a \ttt{Shutdown()} system call to deal with shutdown upon
user request in a safe way. Even though we throw away the ability to
return to \ttt{main} after bootstrapping the first task, we are able
to make a copy of the link register and stack pointer in \ttt{main}
very early during boot so that they can be restored and so that we can
return to RedBoot in a consistent manner.

\newpage
\section*{Demonstration Output}
\verbatiminput{k3_output.txt}

\newpage
\begin{description}
\item[Lines 1-5] \hfill \\
These are the standard starting up messages. Since the clock server is
in the process of initialization there isn't a notifier to actually
wait on the clock ticks causing the missed tick messages to be thrown
until the server has started. Once all the servers have started the
task launcher polls for user input.

\item[Lines 6-11] \hfill \\
These are the starting messages of the root launching task for the K3
demo. First the demo task show that it has stated and the ids of the 4
children tasks that have the inputs in the same order as detailed by
the assignment specification. Finally the task shows that is has gone
into a blocking mode while it waits for its children tasks to
terminate.

\item[Line 12] \hfill \\
K3\_Root is blocked on waiting for its children tasks to notify it
that they are done. All of the K3\_Tasks are waiting for the clock
server to let them know that their delay time has passed. Task
launcher gets scheduled again and writes out its message to notify
that it is polling for more input.

\item[Lines 13-51] \hfill \\
The messages that the children tasks of the k3 launcher task. The
order of their messages follows the value calculated by
$Delay * Iter$. This order is the expected output and shows that the clock
server is waking up tasks in a correct timing with relation to the
other tasks currently in the system.

\item[Line 51] \hfill \\
All of the children tasks are done running to the demo task is now
going to call \ttt{Exit()}.

\end{description}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
